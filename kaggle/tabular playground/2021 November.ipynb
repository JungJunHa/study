{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGnkJuR1C2UY",
        "outputId": "9e09356e-919e-4c16-ae80-b7e08e0ea2f3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import gc\n",
        "import lightgbm as lgb\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj04XG26Hu4L"
      },
      "source": [
        "from scipy.stats import norm, probplot, skew\n",
        "from scipy.special import boxcox1p\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, PolynomialFeatures, RobustScaler\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import KFold, GridSearchCV, train_test_split, cross_val_score\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
        "from sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
        "from sklearn.ensemble import RandomForestRegressor as rfr, GradientBoostingRegressor as gbr\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from mlxtend.regressor import StackingCVRegressor\n",
        "\n",
        "from functools import partial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSZIuVtEDAeN"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/빅데이터 분석 실습/[실전 과제] Kaggle playground/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/빅데이터 분석 실습/[실전 과제] Kaggle playground/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "dmqFYVofDxU4",
        "outputId": "25e6861a-0d02-4223-c3e8-ec4bf8c7087b"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>f0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>f10</th>\n",
              "      <th>f11</th>\n",
              "      <th>f12</th>\n",
              "      <th>f13</th>\n",
              "      <th>f14</th>\n",
              "      <th>f15</th>\n",
              "      <th>f16</th>\n",
              "      <th>f17</th>\n",
              "      <th>f18</th>\n",
              "      <th>f19</th>\n",
              "      <th>f20</th>\n",
              "      <th>f21</th>\n",
              "      <th>f22</th>\n",
              "      <th>f23</th>\n",
              "      <th>f24</th>\n",
              "      <th>f25</th>\n",
              "      <th>f26</th>\n",
              "      <th>f27</th>\n",
              "      <th>f28</th>\n",
              "      <th>f29</th>\n",
              "      <th>f30</th>\n",
              "      <th>f31</th>\n",
              "      <th>f32</th>\n",
              "      <th>f33</th>\n",
              "      <th>f34</th>\n",
              "      <th>f35</th>\n",
              "      <th>f36</th>\n",
              "      <th>f37</th>\n",
              "      <th>f38</th>\n",
              "      <th>...</th>\n",
              "      <th>f61</th>\n",
              "      <th>f62</th>\n",
              "      <th>f63</th>\n",
              "      <th>f64</th>\n",
              "      <th>f65</th>\n",
              "      <th>f66</th>\n",
              "      <th>f67</th>\n",
              "      <th>f68</th>\n",
              "      <th>f69</th>\n",
              "      <th>f70</th>\n",
              "      <th>f71</th>\n",
              "      <th>f72</th>\n",
              "      <th>f73</th>\n",
              "      <th>f74</th>\n",
              "      <th>f75</th>\n",
              "      <th>f76</th>\n",
              "      <th>f77</th>\n",
              "      <th>f78</th>\n",
              "      <th>f79</th>\n",
              "      <th>f80</th>\n",
              "      <th>f81</th>\n",
              "      <th>f82</th>\n",
              "      <th>f83</th>\n",
              "      <th>f84</th>\n",
              "      <th>f85</th>\n",
              "      <th>f86</th>\n",
              "      <th>f87</th>\n",
              "      <th>f88</th>\n",
              "      <th>f89</th>\n",
              "      <th>f90</th>\n",
              "      <th>f91</th>\n",
              "      <th>f92</th>\n",
              "      <th>f93</th>\n",
              "      <th>f94</th>\n",
              "      <th>f95</th>\n",
              "      <th>f96</th>\n",
              "      <th>f97</th>\n",
              "      <th>f98</th>\n",
              "      <th>f99</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.106643</td>\n",
              "      <td>3.59437</td>\n",
              "      <td>132.8040</td>\n",
              "      <td>3.18428</td>\n",
              "      <td>0.081971</td>\n",
              "      <td>1.18859</td>\n",
              "      <td>3.73238</td>\n",
              "      <td>2.266270</td>\n",
              "      <td>2.09959</td>\n",
              "      <td>0.012330</td>\n",
              "      <td>1.607190</td>\n",
              "      <td>-0.318058</td>\n",
              "      <td>0.560137</td>\n",
              "      <td>2.806880</td>\n",
              "      <td>1.35114</td>\n",
              "      <td>2.535930</td>\n",
              "      <td>0.197527</td>\n",
              "      <td>0.676494</td>\n",
              "      <td>1.98979</td>\n",
              "      <td>-3.842450</td>\n",
              "      <td>0.037380</td>\n",
              "      <td>0.230322</td>\n",
              "      <td>3.33055</td>\n",
              "      <td>0.009397</td>\n",
              "      <td>0.144738</td>\n",
              "      <td>3.05131</td>\n",
              "      <td>1.30362</td>\n",
              "      <td>0.033225</td>\n",
              "      <td>-0.018284</td>\n",
              "      <td>2.748210</td>\n",
              "      <td>-0.009294</td>\n",
              "      <td>-0.036271</td>\n",
              "      <td>-0.049871</td>\n",
              "      <td>0.019484</td>\n",
              "      <td>3.898460</td>\n",
              "      <td>11.2863</td>\n",
              "      <td>1.138020</td>\n",
              "      <td>3.366880</td>\n",
              "      <td>4.94446</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.027551</td>\n",
              "      <td>0.019483</td>\n",
              "      <td>-0.048826</td>\n",
              "      <td>0.050748</td>\n",
              "      <td>3.729300</td>\n",
              "      <td>5.017440</td>\n",
              "      <td>4.186880</td>\n",
              "      <td>0.063342</td>\n",
              "      <td>0.121043</td>\n",
              "      <td>1.37175</td>\n",
              "      <td>4.017450</td>\n",
              "      <td>0.167613</td>\n",
              "      <td>0.039754</td>\n",
              "      <td>2.042360</td>\n",
              "      <td>-0.016614</td>\n",
              "      <td>0.107679</td>\n",
              "      <td>3.507250</td>\n",
              "      <td>0.013660</td>\n",
              "      <td>-0.097023</td>\n",
              "      <td>5.396070</td>\n",
              "      <td>0.244457</td>\n",
              "      <td>3.49184</td>\n",
              "      <td>0.113090</td>\n",
              "      <td>-0.015472</td>\n",
              "      <td>4.208790</td>\n",
              "      <td>4.106560</td>\n",
              "      <td>0.037227</td>\n",
              "      <td>-0.118814</td>\n",
              "      <td>0.067086</td>\n",
              "      <td>0.010739</td>\n",
              "      <td>1.09862</td>\n",
              "      <td>0.013331</td>\n",
              "      <td>-0.011715</td>\n",
              "      <td>0.052759</td>\n",
              "      <td>0.065400</td>\n",
              "      <td>4.211250</td>\n",
              "      <td>1.97877</td>\n",
              "      <td>0.085974</td>\n",
              "      <td>0.240496</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.125021</td>\n",
              "      <td>1.67336</td>\n",
              "      <td>76.5336</td>\n",
              "      <td>3.37825</td>\n",
              "      <td>0.099400</td>\n",
              "      <td>5.09366</td>\n",
              "      <td>1.27562</td>\n",
              "      <td>-0.471318</td>\n",
              "      <td>4.54594</td>\n",
              "      <td>0.037706</td>\n",
              "      <td>0.331749</td>\n",
              "      <td>0.325091</td>\n",
              "      <td>0.062040</td>\n",
              "      <td>2.262150</td>\n",
              "      <td>4.33943</td>\n",
              "      <td>-0.224999</td>\n",
              "      <td>0.233586</td>\n",
              "      <td>3.381280</td>\n",
              "      <td>1.90299</td>\n",
              "      <td>0.067874</td>\n",
              "      <td>-0.051268</td>\n",
              "      <td>0.006135</td>\n",
              "      <td>2.60444</td>\n",
              "      <td>0.103441</td>\n",
              "      <td>0.067638</td>\n",
              "      <td>4.75362</td>\n",
              "      <td>1.85552</td>\n",
              "      <td>-0.181834</td>\n",
              "      <td>0.008359</td>\n",
              "      <td>3.166340</td>\n",
              "      <td>0.011850</td>\n",
              "      <td>0.022292</td>\n",
              "      <td>0.069320</td>\n",
              "      <td>0.117109</td>\n",
              "      <td>0.315276</td>\n",
              "      <td>24.4807</td>\n",
              "      <td>1.672270</td>\n",
              "      <td>-0.409067</td>\n",
              "      <td>4.95475</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.010841</td>\n",
              "      <td>0.064584</td>\n",
              "      <td>0.102548</td>\n",
              "      <td>0.093611</td>\n",
              "      <td>0.964089</td>\n",
              "      <td>0.630422</td>\n",
              "      <td>4.307340</td>\n",
              "      <td>0.091289</td>\n",
              "      <td>-0.036360</td>\n",
              "      <td>3.61767</td>\n",
              "      <td>3.103240</td>\n",
              "      <td>0.000657</td>\n",
              "      <td>0.051302</td>\n",
              "      <td>1.924620</td>\n",
              "      <td>0.123294</td>\n",
              "      <td>-0.022671</td>\n",
              "      <td>1.548120</td>\n",
              "      <td>-0.010397</td>\n",
              "      <td>0.058330</td>\n",
              "      <td>3.661310</td>\n",
              "      <td>-0.118386</td>\n",
              "      <td>2.35739</td>\n",
              "      <td>-0.009112</td>\n",
              "      <td>0.178701</td>\n",
              "      <td>4.097350</td>\n",
              "      <td>3.532890</td>\n",
              "      <td>0.005244</td>\n",
              "      <td>0.121381</td>\n",
              "      <td>0.109968</td>\n",
              "      <td>0.135838</td>\n",
              "      <td>3.46017</td>\n",
              "      <td>0.017054</td>\n",
              "      <td>0.124863</td>\n",
              "      <td>0.154064</td>\n",
              "      <td>0.606848</td>\n",
              "      <td>-0.267928</td>\n",
              "      <td>2.57786</td>\n",
              "      <td>-0.020877</td>\n",
              "      <td>0.024719</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.036330</td>\n",
              "      <td>1.49747</td>\n",
              "      <td>233.5460</td>\n",
              "      <td>2.19435</td>\n",
              "      <td>0.026914</td>\n",
              "      <td>3.12694</td>\n",
              "      <td>5.05687</td>\n",
              "      <td>3.849460</td>\n",
              "      <td>1.80187</td>\n",
              "      <td>0.056995</td>\n",
              "      <td>0.328684</td>\n",
              "      <td>2.968810</td>\n",
              "      <td>0.105244</td>\n",
              "      <td>2.069490</td>\n",
              "      <td>5.30986</td>\n",
              "      <td>1.354790</td>\n",
              "      <td>-0.262018</td>\n",
              "      <td>1.379080</td>\n",
              "      <td>1.48091</td>\n",
              "      <td>0.020542</td>\n",
              "      <td>-0.008806</td>\n",
              "      <td>0.109348</td>\n",
              "      <td>1.68365</td>\n",
              "      <td>0.038180</td>\n",
              "      <td>0.123716</td>\n",
              "      <td>1.11248</td>\n",
              "      <td>3.57166</td>\n",
              "      <td>0.120601</td>\n",
              "      <td>0.082069</td>\n",
              "      <td>2.233520</td>\n",
              "      <td>0.002270</td>\n",
              "      <td>0.045182</td>\n",
              "      <td>0.014405</td>\n",
              "      <td>0.011599</td>\n",
              "      <td>-0.502849</td>\n",
              "      <td>33.7382</td>\n",
              "      <td>1.417500</td>\n",
              "      <td>1.071350</td>\n",
              "      <td>3.22296</td>\n",
              "      <td>...</td>\n",
              "      <td>0.027571</td>\n",
              "      <td>-0.007121</td>\n",
              "      <td>-0.048914</td>\n",
              "      <td>-0.002574</td>\n",
              "      <td>1.865090</td>\n",
              "      <td>2.404170</td>\n",
              "      <td>0.411741</td>\n",
              "      <td>0.057749</td>\n",
              "      <td>0.525174</td>\n",
              "      <td>2.16879</td>\n",
              "      <td>0.828297</td>\n",
              "      <td>0.089848</td>\n",
              "      <td>0.093744</td>\n",
              "      <td>4.949010</td>\n",
              "      <td>-0.010978</td>\n",
              "      <td>0.076671</td>\n",
              "      <td>0.266784</td>\n",
              "      <td>0.038691</td>\n",
              "      <td>0.382731</td>\n",
              "      <td>3.847600</td>\n",
              "      <td>-0.121482</td>\n",
              "      <td>3.74084</td>\n",
              "      <td>0.147098</td>\n",
              "      <td>-0.016566</td>\n",
              "      <td>0.614651</td>\n",
              "      <td>2.125840</td>\n",
              "      <td>0.078828</td>\n",
              "      <td>0.979808</td>\n",
              "      <td>0.026758</td>\n",
              "      <td>0.117310</td>\n",
              "      <td>4.88300</td>\n",
              "      <td>0.085222</td>\n",
              "      <td>0.032396</td>\n",
              "      <td>0.116092</td>\n",
              "      <td>-0.001689</td>\n",
              "      <td>-0.520069</td>\n",
              "      <td>2.14112</td>\n",
              "      <td>0.124464</td>\n",
              "      <td>0.148209</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-0.014077</td>\n",
              "      <td>0.24600</td>\n",
              "      <td>779.9670</td>\n",
              "      <td>1.89064</td>\n",
              "      <td>0.006948</td>\n",
              "      <td>1.53112</td>\n",
              "      <td>2.69800</td>\n",
              "      <td>4.517330</td>\n",
              "      <td>4.50332</td>\n",
              "      <td>0.123494</td>\n",
              "      <td>1.002680</td>\n",
              "      <td>4.869600</td>\n",
              "      <td>0.058411</td>\n",
              "      <td>2.497850</td>\n",
              "      <td>1.23843</td>\n",
              "      <td>2.348360</td>\n",
              "      <td>0.175475</td>\n",
              "      <td>1.608890</td>\n",
              "      <td>2.02881</td>\n",
              "      <td>0.042086</td>\n",
              "      <td>0.005141</td>\n",
              "      <td>0.076506</td>\n",
              "      <td>1.65122</td>\n",
              "      <td>0.111813</td>\n",
              "      <td>0.121641</td>\n",
              "      <td>0.58912</td>\n",
              "      <td>4.23692</td>\n",
              "      <td>-0.032843</td>\n",
              "      <td>0.058168</td>\n",
              "      <td>0.712927</td>\n",
              "      <td>0.097465</td>\n",
              "      <td>0.072744</td>\n",
              "      <td>0.000324</td>\n",
              "      <td>0.063362</td>\n",
              "      <td>4.063820</td>\n",
              "      <td>25.3824</td>\n",
              "      <td>0.576572</td>\n",
              "      <td>2.026210</td>\n",
              "      <td>2.96843</td>\n",
              "      <td>...</td>\n",
              "      <td>0.110884</td>\n",
              "      <td>0.026837</td>\n",
              "      <td>2.931160</td>\n",
              "      <td>0.068112</td>\n",
              "      <td>-0.495192</td>\n",
              "      <td>1.345280</td>\n",
              "      <td>2.242750</td>\n",
              "      <td>0.035611</td>\n",
              "      <td>-0.139274</td>\n",
              "      <td>4.74243</td>\n",
              "      <td>3.292740</td>\n",
              "      <td>0.117877</td>\n",
              "      <td>0.065605</td>\n",
              "      <td>0.556711</td>\n",
              "      <td>-0.058029</td>\n",
              "      <td>0.070501</td>\n",
              "      <td>1.101250</td>\n",
              "      <td>0.068559</td>\n",
              "      <td>0.162928</td>\n",
              "      <td>4.070180</td>\n",
              "      <td>-0.008835</td>\n",
              "      <td>3.89678</td>\n",
              "      <td>0.913739</td>\n",
              "      <td>-0.163204</td>\n",
              "      <td>3.074850</td>\n",
              "      <td>4.356780</td>\n",
              "      <td>-0.048894</td>\n",
              "      <td>4.917990</td>\n",
              "      <td>0.069930</td>\n",
              "      <td>-0.015347</td>\n",
              "      <td>3.47439</td>\n",
              "      <td>-0.017103</td>\n",
              "      <td>-0.008100</td>\n",
              "      <td>0.062013</td>\n",
              "      <td>0.041193</td>\n",
              "      <td>0.511657</td>\n",
              "      <td>1.96860</td>\n",
              "      <td>0.040017</td>\n",
              "      <td>0.044873</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-0.003259</td>\n",
              "      <td>3.71542</td>\n",
              "      <td>156.1280</td>\n",
              "      <td>2.14772</td>\n",
              "      <td>0.018284</td>\n",
              "      <td>2.09859</td>\n",
              "      <td>4.15492</td>\n",
              "      <td>-0.038236</td>\n",
              "      <td>3.37145</td>\n",
              "      <td>0.034166</td>\n",
              "      <td>0.711483</td>\n",
              "      <td>0.769988</td>\n",
              "      <td>0.057555</td>\n",
              "      <td>0.957257</td>\n",
              "      <td>3.71145</td>\n",
              "      <td>5.464350</td>\n",
              "      <td>0.287104</td>\n",
              "      <td>2.616950</td>\n",
              "      <td>1.38403</td>\n",
              "      <td>0.074883</td>\n",
              "      <td>-0.010543</td>\n",
              "      <td>0.109121</td>\n",
              "      <td>2.27602</td>\n",
              "      <td>0.008023</td>\n",
              "      <td>0.045236</td>\n",
              "      <td>4.35954</td>\n",
              "      <td>5.07562</td>\n",
              "      <td>-0.009376</td>\n",
              "      <td>0.528966</td>\n",
              "      <td>4.053350</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.106828</td>\n",
              "      <td>0.051307</td>\n",
              "      <td>0.045939</td>\n",
              "      <td>3.402460</td>\n",
              "      <td>15.5615</td>\n",
              "      <td>1.635960</td>\n",
              "      <td>0.047029</td>\n",
              "      <td>4.01771</td>\n",
              "      <td>...</td>\n",
              "      <td>0.075586</td>\n",
              "      <td>0.032114</td>\n",
              "      <td>-0.042284</td>\n",
              "      <td>0.047974</td>\n",
              "      <td>-0.294184</td>\n",
              "      <td>5.065600</td>\n",
              "      <td>1.050290</td>\n",
              "      <td>0.034019</td>\n",
              "      <td>0.024611</td>\n",
              "      <td>3.12578</td>\n",
              "      <td>2.262840</td>\n",
              "      <td>0.082462</td>\n",
              "      <td>-0.023296</td>\n",
              "      <td>5.615850</td>\n",
              "      <td>0.086238</td>\n",
              "      <td>0.157568</td>\n",
              "      <td>3.725670</td>\n",
              "      <td>0.061247</td>\n",
              "      <td>0.086603</td>\n",
              "      <td>0.607246</td>\n",
              "      <td>1.411090</td>\n",
              "      <td>2.06062</td>\n",
              "      <td>-0.023154</td>\n",
              "      <td>0.011234</td>\n",
              "      <td>2.155530</td>\n",
              "      <td>0.914518</td>\n",
              "      <td>0.044521</td>\n",
              "      <td>0.375731</td>\n",
              "      <td>0.134351</td>\n",
              "      <td>0.013781</td>\n",
              "      <td>1.91059</td>\n",
              "      <td>-0.042943</td>\n",
              "      <td>0.105616</td>\n",
              "      <td>0.125072</td>\n",
              "      <td>0.037509</td>\n",
              "      <td>1.043790</td>\n",
              "      <td>1.07481</td>\n",
              "      <td>-0.012819</td>\n",
              "      <td>0.072798</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 102 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id        f0       f1        f2  ...      f97       f98       f99  target\n",
              "0   0  0.106643  3.59437  132.8040  ...  1.97877  0.085974  0.240496       0\n",
              "1   1  0.125021  1.67336   76.5336  ...  2.57786 -0.020877  0.024719       0\n",
              "2   2  0.036330  1.49747  233.5460  ...  2.14112  0.124464  0.148209       0\n",
              "3   3 -0.014077  0.24600  779.9670  ...  1.96860  0.040017  0.044873       0\n",
              "4   4 -0.003259  3.71542  156.1280  ...  1.07481 -0.012819  0.072798       1\n",
              "\n",
              "[5 rows x 102 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK9cHS07Dy7I",
        "outputId": "34938754-80ee-4b7f-e2fd-18bad58dc377"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(600000, 102)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG9GGGPgD09p",
        "outputId": "16c41a57-cfa9-4d07-92eb-0617d2b11a1f"
      },
      "source": [
        "train.isnull().sum().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3v4hlcLD85m"
      },
      "source": [
        "train.drop('id',axis=1,inplace=True)\n",
        "test.drop('id',axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otWylE9WGOtK"
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(42)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "es = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=20, verbose=0,\n",
        "    mode='min',restore_best_weights=True)\n",
        "\n",
        "\n",
        "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.2, patience=5, verbose=0,\n",
        "    mode='min')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IardqqLZGSN8"
      },
      "source": [
        "colNames=[col for col in test.columns if col not in ['id','f2','f35','f44']]\n",
        "# colNames.remove('id')\n",
        "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
        "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
        "train_nn=train[colNames].copy()\n",
        "test_nn=test[colNames].copy()\n",
        "mm = MinMaxScaler()\n",
        "mm.fit(train[['f2','f35','f44']])\n",
        "train_scaled = mm.transform(train[['f2','f35','f44']])\n",
        "train_scaled = pd.DataFrame(data = train_scaled, columns = ['f2','f35','f44'])\n",
        "mm.fit(test[['f2','f35','f44']])\n",
        "test_scaled = mm.transform(test[['f2','f35','f44']])\n",
        "test_scaled = pd.DataFrame(data = test_scaled, columns = ['f2','f35','f44'])\n",
        "train_nn = pd.concat([train_nn, train_scaled], axis = 1)\n",
        "test_nn = pd.concat([test_nn, test_scaled], axis = 1)\n",
        "train_nn=pd.concat([train_nn,test_nn])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8Tzi39bi4k5",
        "outputId": "c6a36aff-28e1-4ef3-f925-26f66b4d7881"
      },
      "source": [
        "train_nn.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1140000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01qmHg7ajrCo",
        "outputId": "ebb5b530-b22c-435d-d72b-23945a761c5d"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(600000, 101)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWxFCDjMjyik",
        "outputId": "79a11eb9-0cbe-49f5-b09d-3d91f878254a"
      },
      "source": [
        "test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(540000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfHqDDymGU61"
      },
      "source": [
        "train_nn['target']=train['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bUv_LrfGgU8"
      },
      "source": [
        "from keras.backend import sigmoid\n",
        "from sklearn.metrics import roc_auc_score\n",
        "def swish(x, beta = 1):\n",
        "    return (x * sigmoid(beta * x))\n",
        "\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "from keras.layers import Activation\n",
        "get_custom_objects().update({'swish': Activation(swish)})\n",
        "\n",
        "hidden_units = (128,64,32)\n",
        "#initialized number of neurons in each hidden layer\n",
        "\n",
        "def base_model():\n",
        "\n",
        "    num_input = keras.Input(shape=(test.shape[1],), name='num_data')#input layer\n",
        "\n",
        "\n",
        "    out = keras.layers.Concatenate()([num_input])\n",
        "    \n",
        "    # Add one or more hidden layers\n",
        "    for n_hidden in hidden_units:\n",
        "\n",
        "        out = keras.layers.Dense(n_hidden, activation='softmax')(out)\n",
        "        out = keras.layers.Dropout(rate = 0.2)(out)\n",
        "    #out = keras.layers.Concatenate()([out, num_input])\n",
        "\n",
        "    # A single output: our predicted target value probability\n",
        "    out = keras.layers.Dense(1, activation='sigmoid', name='prediction')(out)\n",
        "    \n",
        "    model = keras.Model(\n",
        "    inputs = [num_input],\n",
        "    outputs = out,\n",
        "    )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "n2m0HjWtIBhU",
        "outputId": "e6602c60-9255-433c-d105-548e063cf99f"
      },
      "source": [
        "train_nn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>f10</th>\n",
              "      <th>f11</th>\n",
              "      <th>f12</th>\n",
              "      <th>f13</th>\n",
              "      <th>f14</th>\n",
              "      <th>f15</th>\n",
              "      <th>f16</th>\n",
              "      <th>f17</th>\n",
              "      <th>f18</th>\n",
              "      <th>f19</th>\n",
              "      <th>f20</th>\n",
              "      <th>f21</th>\n",
              "      <th>f22</th>\n",
              "      <th>f23</th>\n",
              "      <th>f24</th>\n",
              "      <th>f25</th>\n",
              "      <th>f26</th>\n",
              "      <th>f27</th>\n",
              "      <th>f28</th>\n",
              "      <th>f29</th>\n",
              "      <th>f30</th>\n",
              "      <th>f31</th>\n",
              "      <th>f32</th>\n",
              "      <th>f33</th>\n",
              "      <th>f34</th>\n",
              "      <th>f36</th>\n",
              "      <th>f37</th>\n",
              "      <th>f38</th>\n",
              "      <th>f39</th>\n",
              "      <th>f40</th>\n",
              "      <th>f41</th>\n",
              "      <th>...</th>\n",
              "      <th>f64</th>\n",
              "      <th>f65</th>\n",
              "      <th>f66</th>\n",
              "      <th>f67</th>\n",
              "      <th>f68</th>\n",
              "      <th>f69</th>\n",
              "      <th>f70</th>\n",
              "      <th>f71</th>\n",
              "      <th>f72</th>\n",
              "      <th>f73</th>\n",
              "      <th>f74</th>\n",
              "      <th>f75</th>\n",
              "      <th>f76</th>\n",
              "      <th>f77</th>\n",
              "      <th>f78</th>\n",
              "      <th>f79</th>\n",
              "      <th>f80</th>\n",
              "      <th>f81</th>\n",
              "      <th>f82</th>\n",
              "      <th>f83</th>\n",
              "      <th>f84</th>\n",
              "      <th>f85</th>\n",
              "      <th>f86</th>\n",
              "      <th>f87</th>\n",
              "      <th>f88</th>\n",
              "      <th>f89</th>\n",
              "      <th>f90</th>\n",
              "      <th>f91</th>\n",
              "      <th>f92</th>\n",
              "      <th>f93</th>\n",
              "      <th>f94</th>\n",
              "      <th>f95</th>\n",
              "      <th>f96</th>\n",
              "      <th>f97</th>\n",
              "      <th>f98</th>\n",
              "      <th>f99</th>\n",
              "      <th>f2</th>\n",
              "      <th>f35</th>\n",
              "      <th>f44</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.106643</td>\n",
              "      <td>3.59437</td>\n",
              "      <td>3.184280</td>\n",
              "      <td>0.081971</td>\n",
              "      <td>1.188590</td>\n",
              "      <td>3.732380</td>\n",
              "      <td>2.266270</td>\n",
              "      <td>2.09959</td>\n",
              "      <td>0.012330</td>\n",
              "      <td>1.607190</td>\n",
              "      <td>-0.318058</td>\n",
              "      <td>0.560137</td>\n",
              "      <td>2.806880</td>\n",
              "      <td>1.35114</td>\n",
              "      <td>2.535930</td>\n",
              "      <td>0.197527</td>\n",
              "      <td>0.676494</td>\n",
              "      <td>1.98979</td>\n",
              "      <td>-3.842450</td>\n",
              "      <td>0.037380</td>\n",
              "      <td>0.230322</td>\n",
              "      <td>3.330550</td>\n",
              "      <td>0.009397</td>\n",
              "      <td>0.144738</td>\n",
              "      <td>3.051310</td>\n",
              "      <td>1.303620</td>\n",
              "      <td>0.033225</td>\n",
              "      <td>-0.018284</td>\n",
              "      <td>2.748210</td>\n",
              "      <td>-0.009294</td>\n",
              "      <td>-0.036271</td>\n",
              "      <td>-0.049871</td>\n",
              "      <td>0.019484</td>\n",
              "      <td>3.898460</td>\n",
              "      <td>1.138020</td>\n",
              "      <td>3.366880</td>\n",
              "      <td>4.94446</td>\n",
              "      <td>-0.105772</td>\n",
              "      <td>2.11345</td>\n",
              "      <td>3.452230</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050748</td>\n",
              "      <td>3.729300</td>\n",
              "      <td>5.017440</td>\n",
              "      <td>4.186880</td>\n",
              "      <td>0.063342</td>\n",
              "      <td>0.121043</td>\n",
              "      <td>1.37175</td>\n",
              "      <td>4.017450</td>\n",
              "      <td>0.167613</td>\n",
              "      <td>0.039754</td>\n",
              "      <td>2.042360</td>\n",
              "      <td>-0.016614</td>\n",
              "      <td>0.107679</td>\n",
              "      <td>3.507250</td>\n",
              "      <td>0.013660</td>\n",
              "      <td>-0.097023</td>\n",
              "      <td>5.396070</td>\n",
              "      <td>0.244457</td>\n",
              "      <td>3.49184</td>\n",
              "      <td>0.113090</td>\n",
              "      <td>-0.015472</td>\n",
              "      <td>4.208790</td>\n",
              "      <td>4.106560</td>\n",
              "      <td>0.037227</td>\n",
              "      <td>-0.118814</td>\n",
              "      <td>0.067086</td>\n",
              "      <td>0.010739</td>\n",
              "      <td>1.098620</td>\n",
              "      <td>0.013331</td>\n",
              "      <td>-0.011715</td>\n",
              "      <td>0.052759</td>\n",
              "      <td>0.065400</td>\n",
              "      <td>4.211250</td>\n",
              "      <td>1.97877</td>\n",
              "      <td>0.085974</td>\n",
              "      <td>0.240496</td>\n",
              "      <td>0.248101</td>\n",
              "      <td>0.219337</td>\n",
              "      <td>0.252487</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.125021</td>\n",
              "      <td>1.67336</td>\n",
              "      <td>3.378250</td>\n",
              "      <td>0.099400</td>\n",
              "      <td>5.093660</td>\n",
              "      <td>1.275620</td>\n",
              "      <td>-0.471318</td>\n",
              "      <td>4.54594</td>\n",
              "      <td>0.037706</td>\n",
              "      <td>0.331749</td>\n",
              "      <td>0.325091</td>\n",
              "      <td>0.062040</td>\n",
              "      <td>2.262150</td>\n",
              "      <td>4.33943</td>\n",
              "      <td>-0.224999</td>\n",
              "      <td>0.233586</td>\n",
              "      <td>3.381280</td>\n",
              "      <td>1.90299</td>\n",
              "      <td>0.067874</td>\n",
              "      <td>-0.051268</td>\n",
              "      <td>0.006135</td>\n",
              "      <td>2.604440</td>\n",
              "      <td>0.103441</td>\n",
              "      <td>0.067638</td>\n",
              "      <td>4.753620</td>\n",
              "      <td>1.855520</td>\n",
              "      <td>-0.181834</td>\n",
              "      <td>0.008359</td>\n",
              "      <td>3.166340</td>\n",
              "      <td>0.011850</td>\n",
              "      <td>0.022292</td>\n",
              "      <td>0.069320</td>\n",
              "      <td>0.117109</td>\n",
              "      <td>0.315276</td>\n",
              "      <td>1.672270</td>\n",
              "      <td>-0.409067</td>\n",
              "      <td>4.95475</td>\n",
              "      <td>0.092358</td>\n",
              "      <td>2.60318</td>\n",
              "      <td>1.954690</td>\n",
              "      <td>...</td>\n",
              "      <td>0.093611</td>\n",
              "      <td>0.964089</td>\n",
              "      <td>0.630422</td>\n",
              "      <td>4.307340</td>\n",
              "      <td>0.091289</td>\n",
              "      <td>-0.036360</td>\n",
              "      <td>3.61767</td>\n",
              "      <td>3.103240</td>\n",
              "      <td>0.000657</td>\n",
              "      <td>0.051302</td>\n",
              "      <td>1.924620</td>\n",
              "      <td>0.123294</td>\n",
              "      <td>-0.022671</td>\n",
              "      <td>1.548120</td>\n",
              "      <td>-0.010397</td>\n",
              "      <td>0.058330</td>\n",
              "      <td>3.661310</td>\n",
              "      <td>-0.118386</td>\n",
              "      <td>2.35739</td>\n",
              "      <td>-0.009112</td>\n",
              "      <td>0.178701</td>\n",
              "      <td>4.097350</td>\n",
              "      <td>3.532890</td>\n",
              "      <td>0.005244</td>\n",
              "      <td>0.121381</td>\n",
              "      <td>0.109968</td>\n",
              "      <td>0.135838</td>\n",
              "      <td>3.460170</td>\n",
              "      <td>0.017054</td>\n",
              "      <td>0.124863</td>\n",
              "      <td>0.154064</td>\n",
              "      <td>0.606848</td>\n",
              "      <td>-0.267928</td>\n",
              "      <td>2.57786</td>\n",
              "      <td>-0.020877</td>\n",
              "      <td>0.024719</td>\n",
              "      <td>0.241034</td>\n",
              "      <td>0.226425</td>\n",
              "      <td>0.258653</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.036330</td>\n",
              "      <td>1.49747</td>\n",
              "      <td>2.194350</td>\n",
              "      <td>0.026914</td>\n",
              "      <td>3.126940</td>\n",
              "      <td>5.056870</td>\n",
              "      <td>3.849460</td>\n",
              "      <td>1.80187</td>\n",
              "      <td>0.056995</td>\n",
              "      <td>0.328684</td>\n",
              "      <td>2.968810</td>\n",
              "      <td>0.105244</td>\n",
              "      <td>2.069490</td>\n",
              "      <td>5.30986</td>\n",
              "      <td>1.354790</td>\n",
              "      <td>-0.262018</td>\n",
              "      <td>1.379080</td>\n",
              "      <td>1.48091</td>\n",
              "      <td>0.020542</td>\n",
              "      <td>-0.008806</td>\n",
              "      <td>0.109348</td>\n",
              "      <td>1.683650</td>\n",
              "      <td>0.038180</td>\n",
              "      <td>0.123716</td>\n",
              "      <td>1.112480</td>\n",
              "      <td>3.571660</td>\n",
              "      <td>0.120601</td>\n",
              "      <td>0.082069</td>\n",
              "      <td>2.233520</td>\n",
              "      <td>0.002270</td>\n",
              "      <td>0.045182</td>\n",
              "      <td>0.014405</td>\n",
              "      <td>0.011599</td>\n",
              "      <td>-0.502849</td>\n",
              "      <td>1.417500</td>\n",
              "      <td>1.071350</td>\n",
              "      <td>3.22296</td>\n",
              "      <td>2.122030</td>\n",
              "      <td>3.08216</td>\n",
              "      <td>0.637555</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002574</td>\n",
              "      <td>1.865090</td>\n",
              "      <td>2.404170</td>\n",
              "      <td>0.411741</td>\n",
              "      <td>0.057749</td>\n",
              "      <td>0.525174</td>\n",
              "      <td>2.16879</td>\n",
              "      <td>0.828297</td>\n",
              "      <td>0.089848</td>\n",
              "      <td>0.093744</td>\n",
              "      <td>4.949010</td>\n",
              "      <td>-0.010978</td>\n",
              "      <td>0.076671</td>\n",
              "      <td>0.266784</td>\n",
              "      <td>0.038691</td>\n",
              "      <td>0.382731</td>\n",
              "      <td>3.847600</td>\n",
              "      <td>-0.121482</td>\n",
              "      <td>3.74084</td>\n",
              "      <td>0.147098</td>\n",
              "      <td>-0.016566</td>\n",
              "      <td>0.614651</td>\n",
              "      <td>2.125840</td>\n",
              "      <td>0.078828</td>\n",
              "      <td>0.979808</td>\n",
              "      <td>0.026758</td>\n",
              "      <td>0.117310</td>\n",
              "      <td>4.883000</td>\n",
              "      <td>0.085222</td>\n",
              "      <td>0.032396</td>\n",
              "      <td>0.116092</td>\n",
              "      <td>-0.001689</td>\n",
              "      <td>-0.520069</td>\n",
              "      <td>2.14112</td>\n",
              "      <td>0.124464</td>\n",
              "      <td>0.148209</td>\n",
              "      <td>0.260754</td>\n",
              "      <td>0.231398</td>\n",
              "      <td>0.343598</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.014077</td>\n",
              "      <td>0.24600</td>\n",
              "      <td>1.890640</td>\n",
              "      <td>0.006948</td>\n",
              "      <td>1.531120</td>\n",
              "      <td>2.698000</td>\n",
              "      <td>4.517330</td>\n",
              "      <td>4.50332</td>\n",
              "      <td>0.123494</td>\n",
              "      <td>1.002680</td>\n",
              "      <td>4.869600</td>\n",
              "      <td>0.058411</td>\n",
              "      <td>2.497850</td>\n",
              "      <td>1.23843</td>\n",
              "      <td>2.348360</td>\n",
              "      <td>0.175475</td>\n",
              "      <td>1.608890</td>\n",
              "      <td>2.02881</td>\n",
              "      <td>0.042086</td>\n",
              "      <td>0.005141</td>\n",
              "      <td>0.076506</td>\n",
              "      <td>1.651220</td>\n",
              "      <td>0.111813</td>\n",
              "      <td>0.121641</td>\n",
              "      <td>0.589120</td>\n",
              "      <td>4.236920</td>\n",
              "      <td>-0.032843</td>\n",
              "      <td>0.058168</td>\n",
              "      <td>0.712927</td>\n",
              "      <td>0.097465</td>\n",
              "      <td>0.072744</td>\n",
              "      <td>0.000324</td>\n",
              "      <td>0.063362</td>\n",
              "      <td>4.063820</td>\n",
              "      <td>0.576572</td>\n",
              "      <td>2.026210</td>\n",
              "      <td>2.96843</td>\n",
              "      <td>1.085670</td>\n",
              "      <td>1.71088</td>\n",
              "      <td>1.371820</td>\n",
              "      <td>...</td>\n",
              "      <td>0.068112</td>\n",
              "      <td>-0.495192</td>\n",
              "      <td>1.345280</td>\n",
              "      <td>2.242750</td>\n",
              "      <td>0.035611</td>\n",
              "      <td>-0.139274</td>\n",
              "      <td>4.74243</td>\n",
              "      <td>3.292740</td>\n",
              "      <td>0.117877</td>\n",
              "      <td>0.065605</td>\n",
              "      <td>0.556711</td>\n",
              "      <td>-0.058029</td>\n",
              "      <td>0.070501</td>\n",
              "      <td>1.101250</td>\n",
              "      <td>0.068559</td>\n",
              "      <td>0.162928</td>\n",
              "      <td>4.070180</td>\n",
              "      <td>-0.008835</td>\n",
              "      <td>3.89678</td>\n",
              "      <td>0.913739</td>\n",
              "      <td>-0.163204</td>\n",
              "      <td>3.074850</td>\n",
              "      <td>4.356780</td>\n",
              "      <td>-0.048894</td>\n",
              "      <td>4.917990</td>\n",
              "      <td>0.069930</td>\n",
              "      <td>-0.015347</td>\n",
              "      <td>3.474390</td>\n",
              "      <td>-0.017103</td>\n",
              "      <td>-0.008100</td>\n",
              "      <td>0.062013</td>\n",
              "      <td>0.041193</td>\n",
              "      <td>0.511657</td>\n",
              "      <td>1.96860</td>\n",
              "      <td>0.040017</td>\n",
              "      <td>0.044873</td>\n",
              "      <td>0.329385</td>\n",
              "      <td>0.226910</td>\n",
              "      <td>0.654519</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.003259</td>\n",
              "      <td>3.71542</td>\n",
              "      <td>2.147720</td>\n",
              "      <td>0.018284</td>\n",
              "      <td>2.098590</td>\n",
              "      <td>4.154920</td>\n",
              "      <td>-0.038236</td>\n",
              "      <td>3.37145</td>\n",
              "      <td>0.034166</td>\n",
              "      <td>0.711483</td>\n",
              "      <td>0.769988</td>\n",
              "      <td>0.057555</td>\n",
              "      <td>0.957257</td>\n",
              "      <td>3.71145</td>\n",
              "      <td>5.464350</td>\n",
              "      <td>0.287104</td>\n",
              "      <td>2.616950</td>\n",
              "      <td>1.38403</td>\n",
              "      <td>0.074883</td>\n",
              "      <td>-0.010543</td>\n",
              "      <td>0.109121</td>\n",
              "      <td>2.276020</td>\n",
              "      <td>0.008023</td>\n",
              "      <td>0.045236</td>\n",
              "      <td>4.359540</td>\n",
              "      <td>5.075620</td>\n",
              "      <td>-0.009376</td>\n",
              "      <td>0.528966</td>\n",
              "      <td>4.053350</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.106828</td>\n",
              "      <td>0.051307</td>\n",
              "      <td>0.045939</td>\n",
              "      <td>3.402460</td>\n",
              "      <td>1.635960</td>\n",
              "      <td>0.047029</td>\n",
              "      <td>4.01771</td>\n",
              "      <td>0.155748</td>\n",
              "      <td>5.28998</td>\n",
              "      <td>4.118920</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047974</td>\n",
              "      <td>-0.294184</td>\n",
              "      <td>5.065600</td>\n",
              "      <td>1.050290</td>\n",
              "      <td>0.034019</td>\n",
              "      <td>0.024611</td>\n",
              "      <td>3.12578</td>\n",
              "      <td>2.262840</td>\n",
              "      <td>0.082462</td>\n",
              "      <td>-0.023296</td>\n",
              "      <td>5.615850</td>\n",
              "      <td>0.086238</td>\n",
              "      <td>0.157568</td>\n",
              "      <td>3.725670</td>\n",
              "      <td>0.061247</td>\n",
              "      <td>0.086603</td>\n",
              "      <td>0.607246</td>\n",
              "      <td>1.411090</td>\n",
              "      <td>2.06062</td>\n",
              "      <td>-0.023154</td>\n",
              "      <td>0.011234</td>\n",
              "      <td>2.155530</td>\n",
              "      <td>0.914518</td>\n",
              "      <td>0.044521</td>\n",
              "      <td>0.375731</td>\n",
              "      <td>0.134351</td>\n",
              "      <td>0.013781</td>\n",
              "      <td>1.910590</td>\n",
              "      <td>-0.042943</td>\n",
              "      <td>0.105616</td>\n",
              "      <td>0.125072</td>\n",
              "      <td>0.037509</td>\n",
              "      <td>1.043790</td>\n",
              "      <td>1.07481</td>\n",
              "      <td>-0.012819</td>\n",
              "      <td>0.072798</td>\n",
              "      <td>0.251031</td>\n",
              "      <td>0.221634</td>\n",
              "      <td>0.262139</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539995</th>\n",
              "      <td>0.431599</td>\n",
              "      <td>1.50756</td>\n",
              "      <td>2.928480</td>\n",
              "      <td>2.788830</td>\n",
              "      <td>5.152330</td>\n",
              "      <td>2.779980</td>\n",
              "      <td>0.816389</td>\n",
              "      <td>4.79156</td>\n",
              "      <td>0.026331</td>\n",
              "      <td>4.562970</td>\n",
              "      <td>0.233379</td>\n",
              "      <td>0.254225</td>\n",
              "      <td>2.213830</td>\n",
              "      <td>5.58360</td>\n",
              "      <td>3.800710</td>\n",
              "      <td>0.503562</td>\n",
              "      <td>4.061380</td>\n",
              "      <td>2.75466</td>\n",
              "      <td>0.112982</td>\n",
              "      <td>-0.038411</td>\n",
              "      <td>0.077714</td>\n",
              "      <td>3.010950</td>\n",
              "      <td>-0.008242</td>\n",
              "      <td>0.043315</td>\n",
              "      <td>0.080434</td>\n",
              "      <td>2.371080</td>\n",
              "      <td>0.133727</td>\n",
              "      <td>0.355986</td>\n",
              "      <td>2.014700</td>\n",
              "      <td>0.002188</td>\n",
              "      <td>0.220446</td>\n",
              "      <td>0.078856</td>\n",
              "      <td>0.084574</td>\n",
              "      <td>0.758547</td>\n",
              "      <td>2.367110</td>\n",
              "      <td>0.179322</td>\n",
              "      <td>1.25804</td>\n",
              "      <td>0.029281</td>\n",
              "      <td>3.73639</td>\n",
              "      <td>2.634860</td>\n",
              "      <td>...</td>\n",
              "      <td>0.036363</td>\n",
              "      <td>1.208520</td>\n",
              "      <td>3.991380</td>\n",
              "      <td>3.872910</td>\n",
              "      <td>-0.004119</td>\n",
              "      <td>-0.040713</td>\n",
              "      <td>3.12179</td>\n",
              "      <td>1.248680</td>\n",
              "      <td>-0.005633</td>\n",
              "      <td>-0.001866</td>\n",
              "      <td>5.244440</td>\n",
              "      <td>-0.008260</td>\n",
              "      <td>0.015797</td>\n",
              "      <td>0.478205</td>\n",
              "      <td>0.040919</td>\n",
              "      <td>0.118144</td>\n",
              "      <td>4.275970</td>\n",
              "      <td>0.745393</td>\n",
              "      <td>2.59279</td>\n",
              "      <td>0.113751</td>\n",
              "      <td>0.241825</td>\n",
              "      <td>3.706940</td>\n",
              "      <td>4.986650</td>\n",
              "      <td>0.701654</td>\n",
              "      <td>0.619688</td>\n",
              "      <td>0.027862</td>\n",
              "      <td>0.083219</td>\n",
              "      <td>2.545860</td>\n",
              "      <td>0.132701</td>\n",
              "      <td>0.040109</td>\n",
              "      <td>0.121019</td>\n",
              "      <td>0.024841</td>\n",
              "      <td>3.182610</td>\n",
              "      <td>1.25273</td>\n",
              "      <td>0.059733</td>\n",
              "      <td>0.029033</td>\n",
              "      <td>0.227468</td>\n",
              "      <td>0.202863</td>\n",
              "      <td>0.302140</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539996</th>\n",
              "      <td>0.069713</td>\n",
              "      <td>2.35548</td>\n",
              "      <td>2.721580</td>\n",
              "      <td>1.256300</td>\n",
              "      <td>4.248220</td>\n",
              "      <td>2.014550</td>\n",
              "      <td>2.207120</td>\n",
              "      <td>3.02026</td>\n",
              "      <td>0.020398</td>\n",
              "      <td>4.588860</td>\n",
              "      <td>3.219260</td>\n",
              "      <td>-0.011444</td>\n",
              "      <td>2.802410</td>\n",
              "      <td>1.71392</td>\n",
              "      <td>4.173030</td>\n",
              "      <td>-0.028895</td>\n",
              "      <td>2.029860</td>\n",
              "      <td>1.67972</td>\n",
              "      <td>0.059943</td>\n",
              "      <td>0.052139</td>\n",
              "      <td>-0.006159</td>\n",
              "      <td>2.235450</td>\n",
              "      <td>0.105783</td>\n",
              "      <td>0.091786</td>\n",
              "      <td>4.098640</td>\n",
              "      <td>0.232705</td>\n",
              "      <td>0.173007</td>\n",
              "      <td>0.180720</td>\n",
              "      <td>3.947040</td>\n",
              "      <td>0.211080</td>\n",
              "      <td>0.034419</td>\n",
              "      <td>0.077199</td>\n",
              "      <td>0.027838</td>\n",
              "      <td>1.180220</td>\n",
              "      <td>-0.545938</td>\n",
              "      <td>3.045400</td>\n",
              "      <td>2.23775</td>\n",
              "      <td>0.458355</td>\n",
              "      <td>1.36203</td>\n",
              "      <td>3.906970</td>\n",
              "      <td>...</td>\n",
              "      <td>0.072357</td>\n",
              "      <td>0.301745</td>\n",
              "      <td>1.672610</td>\n",
              "      <td>2.853270</td>\n",
              "      <td>0.086909</td>\n",
              "      <td>-0.000318</td>\n",
              "      <td>4.78076</td>\n",
              "      <td>2.296390</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.065691</td>\n",
              "      <td>3.497600</td>\n",
              "      <td>-0.013572</td>\n",
              "      <td>0.092398</td>\n",
              "      <td>1.272370</td>\n",
              "      <td>0.978935</td>\n",
              "      <td>0.723590</td>\n",
              "      <td>2.349420</td>\n",
              "      <td>0.009152</td>\n",
              "      <td>4.54517</td>\n",
              "      <td>0.131280</td>\n",
              "      <td>0.059220</td>\n",
              "      <td>1.718780</td>\n",
              "      <td>3.014750</td>\n",
              "      <td>0.063428</td>\n",
              "      <td>0.048402</td>\n",
              "      <td>0.098273</td>\n",
              "      <td>0.077851</td>\n",
              "      <td>4.019700</td>\n",
              "      <td>0.165319</td>\n",
              "      <td>0.065945</td>\n",
              "      <td>0.081351</td>\n",
              "      <td>-0.022179</td>\n",
              "      <td>4.512380</td>\n",
              "      <td>2.81288</td>\n",
              "      <td>0.016573</td>\n",
              "      <td>0.079498</td>\n",
              "      <td>0.208005</td>\n",
              "      <td>0.203110</td>\n",
              "      <td>0.275294</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539997</th>\n",
              "      <td>0.385075</td>\n",
              "      <td>2.52889</td>\n",
              "      <td>0.975396</td>\n",
              "      <td>0.043852</td>\n",
              "      <td>0.829423</td>\n",
              "      <td>2.014210</td>\n",
              "      <td>1.509500</td>\n",
              "      <td>2.02759</td>\n",
              "      <td>0.097387</td>\n",
              "      <td>3.425350</td>\n",
              "      <td>1.195470</td>\n",
              "      <td>0.090862</td>\n",
              "      <td>-0.130309</td>\n",
              "      <td>5.42106</td>\n",
              "      <td>1.561510</td>\n",
              "      <td>0.132260</td>\n",
              "      <td>0.392255</td>\n",
              "      <td>5.08088</td>\n",
              "      <td>-0.059632</td>\n",
              "      <td>0.074617</td>\n",
              "      <td>0.089685</td>\n",
              "      <td>4.798180</td>\n",
              "      <td>0.092356</td>\n",
              "      <td>0.014446</td>\n",
              "      <td>3.729750</td>\n",
              "      <td>1.649350</td>\n",
              "      <td>-0.005483</td>\n",
              "      <td>0.326276</td>\n",
              "      <td>2.889600</td>\n",
              "      <td>0.067742</td>\n",
              "      <td>0.028514</td>\n",
              "      <td>0.074359</td>\n",
              "      <td>0.070743</td>\n",
              "      <td>1.646070</td>\n",
              "      <td>0.364036</td>\n",
              "      <td>4.478340</td>\n",
              "      <td>3.64816</td>\n",
              "      <td>0.647542</td>\n",
              "      <td>2.16960</td>\n",
              "      <td>3.059480</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003590</td>\n",
              "      <td>4.187090</td>\n",
              "      <td>2.697540</td>\n",
              "      <td>0.594569</td>\n",
              "      <td>-0.000189</td>\n",
              "      <td>-0.005288</td>\n",
              "      <td>4.48377</td>\n",
              "      <td>3.174610</td>\n",
              "      <td>0.453266</td>\n",
              "      <td>0.036806</td>\n",
              "      <td>2.732380</td>\n",
              "      <td>1.071340</td>\n",
              "      <td>0.175437</td>\n",
              "      <td>2.581260</td>\n",
              "      <td>0.031730</td>\n",
              "      <td>0.024921</td>\n",
              "      <td>1.755260</td>\n",
              "      <td>0.757080</td>\n",
              "      <td>2.45946</td>\n",
              "      <td>0.017451</td>\n",
              "      <td>0.352468</td>\n",
              "      <td>2.256030</td>\n",
              "      <td>3.380530</td>\n",
              "      <td>1.079600</td>\n",
              "      <td>1.250540</td>\n",
              "      <td>0.009542</td>\n",
              "      <td>0.060691</td>\n",
              "      <td>3.604950</td>\n",
              "      <td>0.092720</td>\n",
              "      <td>0.067277</td>\n",
              "      <td>0.113736</td>\n",
              "      <td>0.132187</td>\n",
              "      <td>0.567747</td>\n",
              "      <td>-1.11285</td>\n",
              "      <td>0.776967</td>\n",
              "      <td>0.123728</td>\n",
              "      <td>0.216243</td>\n",
              "      <td>0.220541</td>\n",
              "      <td>0.283333</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539998</th>\n",
              "      <td>1.846240</td>\n",
              "      <td>3.41535</td>\n",
              "      <td>-0.120134</td>\n",
              "      <td>0.027113</td>\n",
              "      <td>2.155160</td>\n",
              "      <td>2.529860</td>\n",
              "      <td>2.502250</td>\n",
              "      <td>3.45309</td>\n",
              "      <td>0.090760</td>\n",
              "      <td>1.307160</td>\n",
              "      <td>4.107690</td>\n",
              "      <td>0.034657</td>\n",
              "      <td>0.867694</td>\n",
              "      <td>1.60534</td>\n",
              "      <td>2.978230</td>\n",
              "      <td>0.145505</td>\n",
              "      <td>2.855640</td>\n",
              "      <td>1.13562</td>\n",
              "      <td>0.138793</td>\n",
              "      <td>-0.005488</td>\n",
              "      <td>0.012881</td>\n",
              "      <td>0.576931</td>\n",
              "      <td>0.015822</td>\n",
              "      <td>0.120977</td>\n",
              "      <td>1.147820</td>\n",
              "      <td>0.252155</td>\n",
              "      <td>0.202905</td>\n",
              "      <td>0.125948</td>\n",
              "      <td>3.667900</td>\n",
              "      <td>0.092273</td>\n",
              "      <td>-0.005253</td>\n",
              "      <td>0.029498</td>\n",
              "      <td>0.110625</td>\n",
              "      <td>2.201660</td>\n",
              "      <td>0.181733</td>\n",
              "      <td>1.299380</td>\n",
              "      <td>1.48190</td>\n",
              "      <td>0.229645</td>\n",
              "      <td>3.02903</td>\n",
              "      <td>2.570670</td>\n",
              "      <td>...</td>\n",
              "      <td>0.048100</td>\n",
              "      <td>4.099600</td>\n",
              "      <td>2.931740</td>\n",
              "      <td>3.286140</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>-0.021926</td>\n",
              "      <td>5.91797</td>\n",
              "      <td>4.564490</td>\n",
              "      <td>0.076632</td>\n",
              "      <td>0.064538</td>\n",
              "      <td>0.107602</td>\n",
              "      <td>0.006469</td>\n",
              "      <td>0.012708</td>\n",
              "      <td>1.513980</td>\n",
              "      <td>0.073228</td>\n",
              "      <td>0.044095</td>\n",
              "      <td>3.807870</td>\n",
              "      <td>0.280213</td>\n",
              "      <td>4.00828</td>\n",
              "      <td>0.042557</td>\n",
              "      <td>0.310005</td>\n",
              "      <td>4.949010</td>\n",
              "      <td>3.055720</td>\n",
              "      <td>0.025075</td>\n",
              "      <td>0.154188</td>\n",
              "      <td>0.095477</td>\n",
              "      <td>-0.012514</td>\n",
              "      <td>0.122062</td>\n",
              "      <td>0.099288</td>\n",
              "      <td>0.043613</td>\n",
              "      <td>0.013528</td>\n",
              "      <td>0.078044</td>\n",
              "      <td>-0.337694</td>\n",
              "      <td>0.43673</td>\n",
              "      <td>0.054561</td>\n",
              "      <td>0.082622</td>\n",
              "      <td>0.227795</td>\n",
              "      <td>0.211985</td>\n",
              "      <td>0.257124</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539999</th>\n",
              "      <td>0.475802</td>\n",
              "      <td>2.67074</td>\n",
              "      <td>4.690200</td>\n",
              "      <td>0.069227</td>\n",
              "      <td>5.451320</td>\n",
              "      <td>0.861476</td>\n",
              "      <td>0.854981</td>\n",
              "      <td>5.31520</td>\n",
              "      <td>0.119477</td>\n",
              "      <td>1.972080</td>\n",
              "      <td>3.979380</td>\n",
              "      <td>0.369296</td>\n",
              "      <td>3.342400</td>\n",
              "      <td>2.76147</td>\n",
              "      <td>3.250500</td>\n",
              "      <td>0.056836</td>\n",
              "      <td>4.989740</td>\n",
              "      <td>2.12784</td>\n",
              "      <td>0.062638</td>\n",
              "      <td>0.060752</td>\n",
              "      <td>0.088188</td>\n",
              "      <td>2.084750</td>\n",
              "      <td>0.033652</td>\n",
              "      <td>0.099527</td>\n",
              "      <td>3.948260</td>\n",
              "      <td>2.820770</td>\n",
              "      <td>0.232900</td>\n",
              "      <td>0.075716</td>\n",
              "      <td>3.448480</td>\n",
              "      <td>0.058991</td>\n",
              "      <td>0.058473</td>\n",
              "      <td>0.022776</td>\n",
              "      <td>0.008793</td>\n",
              "      <td>2.960360</td>\n",
              "      <td>-0.359104</td>\n",
              "      <td>3.013510</td>\n",
              "      <td>-0.14368</td>\n",
              "      <td>-0.008140</td>\n",
              "      <td>3.52043</td>\n",
              "      <td>2.306050</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002365</td>\n",
              "      <td>0.830385</td>\n",
              "      <td>4.121860</td>\n",
              "      <td>2.602090</td>\n",
              "      <td>0.113479</td>\n",
              "      <td>-0.036320</td>\n",
              "      <td>3.57042</td>\n",
              "      <td>2.780530</td>\n",
              "      <td>0.004134</td>\n",
              "      <td>0.017726</td>\n",
              "      <td>2.809580</td>\n",
              "      <td>0.157800</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>4.890910</td>\n",
              "      <td>0.084610</td>\n",
              "      <td>0.028233</td>\n",
              "      <td>0.624434</td>\n",
              "      <td>-0.124174</td>\n",
              "      <td>5.00851</td>\n",
              "      <td>0.073614</td>\n",
              "      <td>0.305078</td>\n",
              "      <td>3.123430</td>\n",
              "      <td>4.090430</td>\n",
              "      <td>1.985610</td>\n",
              "      <td>0.086376</td>\n",
              "      <td>0.050968</td>\n",
              "      <td>0.028472</td>\n",
              "      <td>2.054980</td>\n",
              "      <td>0.083090</td>\n",
              "      <td>0.115495</td>\n",
              "      <td>0.143097</td>\n",
              "      <td>0.055605</td>\n",
              "      <td>1.149700</td>\n",
              "      <td>0.28989</td>\n",
              "      <td>0.072163</td>\n",
              "      <td>0.102005</td>\n",
              "      <td>0.273954</td>\n",
              "      <td>0.215971</td>\n",
              "      <td>0.252677</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1140000 rows × 101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              f0       f1        f3  ...       f35       f44  target\n",
              "0       0.106643  3.59437  3.184280  ...  0.219337  0.252487       0\n",
              "1       0.125021  1.67336  3.378250  ...  0.226425  0.258653       0\n",
              "2       0.036330  1.49747  2.194350  ...  0.231398  0.343598       0\n",
              "3      -0.014077  0.24600  1.890640  ...  0.226910  0.654519       0\n",
              "4      -0.003259  3.71542  2.147720  ...  0.221634  0.262139       1\n",
              "...          ...      ...       ...  ...       ...       ...     ...\n",
              "539995  0.431599  1.50756  2.928480  ...  0.202863  0.302140       0\n",
              "539996  0.069713  2.35548  2.721580  ...  0.203110  0.275294       1\n",
              "539997  0.385075  2.52889  0.975396  ...  0.220541  0.283333       0\n",
              "539998  1.846240  3.41535 -0.120134  ...  0.211985  0.257124       0\n",
              "539999  0.475802  2.67074  4.690200  ...  0.215971  0.252677       0\n",
              "\n",
              "[1140000 rows x 101 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSwAWsKMGqWA",
        "outputId": "8171e90e-4cdc-4c23-81e4-6a5354986819"
      },
      "source": [
        "target_name='target'\n",
        "scores_folds = {}\n",
        "model_name = 'NN'\n",
        "pred_name = 'pred_{}'.format(model_name)\n",
        "\n",
        "n_folds = 5\n",
        "kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=2020)\n",
        "scores_folds[model_name] = []\n",
        "counter = 1\n",
        "\n",
        "features_to_consider = list(train_nn)\n",
        "\n",
        "features_to_consider.remove('target')\n",
        "try:\n",
        "    features_to_consider.remove('pred_NN')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "\n",
        "train_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
        "test_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
        "\n",
        "train_nn[pred_name] = 0\n",
        "test_nn[target_name] = 0\n",
        "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
        "\n",
        "for fold, (trn_ind, val_ind) in enumerate(kf.split(train,train['target'])):\n",
        "    print(f'Training fold {fold + 1}')\n",
        "    X_train, X_test = train_nn.iloc[trn_ind][features_to_consider], train_nn.iloc[val_ind][features_to_consider]\n",
        "    y_train, y_test = train_nn.iloc[trn_ind]['target'], train_nn.iloc[val_ind]['target']\n",
        "    print('CV {}/{}'.format(counter, n_folds)) \n",
        "    #############################################################################################\n",
        "    # NN\n",
        "    #############################################################################################\n",
        "    print(X_train.shape,y_train.shape)\n",
        "    print(X_test.shape, y_test.shape)\n",
        "    model = base_model()\n",
        "    \n",
        "    model.compile(\n",
        "        keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics = ['AUC']\n",
        "    )\n",
        "    \n",
        "    num_data = X_train[features_to_consider]\n",
        "    \n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))         \n",
        "    num_data = scaler.fit_transform(num_data.values)    \n",
        "      \n",
        "    target =  y_train\n",
        "    \n",
        "    num_data_test = X_test[features_to_consider]\n",
        "    num_data_test = scaler.transform(num_data_test.values)\n",
        "\n",
        "    model.fit([num_data], \n",
        "              target,               \n",
        "              batch_size=2048,\n",
        "              epochs=1000,\n",
        "              validation_data=([num_data_test], y_test),\n",
        "              callbacks=[es, plateau],\n",
        "              validation_batch_size=len(y_test),\n",
        "              shuffle=True,\n",
        "             verbose = 1)\n",
        "\n",
        "    preds = model.predict([num_data_test]).reshape(1,-1)[0]\n",
        "    score = round(roc_auc_score(y_test,preds),5)\n",
        "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
        "    scores_folds[model_name].append(score)\n",
        "    \n",
        "    tt =scaler.transform(test_nn[features_to_consider].values)\n",
        "    \n",
        "    test_predictions_nn += model.predict([tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds       \n",
        "    counter += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training fold 1\n",
            "CV 1/5\n",
            "(480000, 100) (480000,)\n",
            "(120000, 100) (120000,)\n",
            "Epoch 1/1000\n",
            "235/235 [==============================] - 4s 12ms/step - loss: 0.6933 - auc: 0.5014 - val_loss: 0.6931 - val_auc: 0.5000 - lr: 0.0010\n",
            "Epoch 2/1000\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.6931 - auc: 0.4991 - val_loss: 0.6931 - val_auc: 0.5000 - lr: 0.0010\n",
            "Epoch 3/1000\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.6927 - auc: 0.5338 - val_loss: 0.6907 - val_auc: 0.6899 - lr: 0.0010\n",
            "Epoch 4/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6677 - auc: 0.7093 - val_loss: 0.6329 - val_auc: 0.7306 - lr: 0.0010\n",
            "Epoch 5/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6158 - auc: 0.7262 - val_loss: 0.6051 - val_auc: 0.7329 - lr: 0.0010\n",
            "Epoch 6/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6041 - auc: 0.7305 - val_loss: 0.6011 - val_auc: 0.7341 - lr: 0.0010\n",
            "Epoch 7/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6021 - auc: 0.7318 - val_loss: 0.5998 - val_auc: 0.7350 - lr: 0.0010\n",
            "Epoch 8/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6009 - auc: 0.7328 - val_loss: 0.5988 - val_auc: 0.7359 - lr: 0.0010\n",
            "Epoch 9/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5999 - auc: 0.7336 - val_loss: 0.5978 - val_auc: 0.7365 - lr: 0.0010\n",
            "Epoch 10/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5991 - auc: 0.7344 - val_loss: 0.5977 - val_auc: 0.7372 - lr: 0.0010\n",
            "Epoch 11/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5983 - auc: 0.7349 - val_loss: 0.5963 - val_auc: 0.7377 - lr: 0.0010\n",
            "Epoch 12/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5976 - auc: 0.7354 - val_loss: 0.5961 - val_auc: 0.7382 - lr: 0.0010\n",
            "Epoch 13/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5969 - auc: 0.7360 - val_loss: 0.5954 - val_auc: 0.7386 - lr: 0.0010\n",
            "Epoch 14/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5963 - auc: 0.7365 - val_loss: 0.5944 - val_auc: 0.7392 - lr: 0.0010\n",
            "Epoch 15/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5957 - auc: 0.7369 - val_loss: 0.5939 - val_auc: 0.7397 - lr: 0.0010\n",
            "Epoch 16/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5951 - auc: 0.7374 - val_loss: 0.5939 - val_auc: 0.7399 - lr: 0.0010\n",
            "Epoch 17/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5946 - auc: 0.7377 - val_loss: 0.5929 - val_auc: 0.7404 - lr: 0.0010\n",
            "Epoch 18/1000\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.5940 - auc: 0.7382 - val_loss: 0.5923 - val_auc: 0.7406 - lr: 0.0010\n",
            "Epoch 19/1000\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.5934 - auc: 0.7385 - val_loss: 0.5918 - val_auc: 0.7410 - lr: 0.0010\n",
            "Epoch 20/1000\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.5929 - auc: 0.7389 - val_loss: 0.5913 - val_auc: 0.7413 - lr: 0.0010\n",
            "Epoch 21/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5925 - auc: 0.7391 - val_loss: 0.5908 - val_auc: 0.7417 - lr: 0.0010\n",
            "Epoch 22/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5920 - auc: 0.7395 - val_loss: 0.5903 - val_auc: 0.7420 - lr: 0.0010\n",
            "Epoch 23/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5915 - auc: 0.7398 - val_loss: 0.5899 - val_auc: 0.7422 - lr: 0.0010\n",
            "Epoch 24/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5910 - auc: 0.7401 - val_loss: 0.5896 - val_auc: 0.7425 - lr: 0.0010\n",
            "Epoch 25/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5907 - auc: 0.7403 - val_loss: 0.5894 - val_auc: 0.7429 - lr: 0.0010\n",
            "Epoch 26/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5901 - auc: 0.7406 - val_loss: 0.5889 - val_auc: 0.7431 - lr: 0.0010\n",
            "Epoch 27/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5897 - auc: 0.7409 - val_loss: 0.5883 - val_auc: 0.7433 - lr: 0.0010\n",
            "Epoch 28/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5894 - auc: 0.7411 - val_loss: 0.5880 - val_auc: 0.7435 - lr: 0.0010\n",
            "Epoch 29/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5889 - auc: 0.7414 - val_loss: 0.5880 - val_auc: 0.7438 - lr: 0.0010\n",
            "Epoch 30/1000\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.5885 - auc: 0.7416 - val_loss: 0.5871 - val_auc: 0.7439 - lr: 0.0010\n",
            "Epoch 31/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5882 - auc: 0.7417 - val_loss: 0.5869 - val_auc: 0.7440 - lr: 0.0010\n",
            "Epoch 32/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5877 - auc: 0.7420 - val_loss: 0.5864 - val_auc: 0.7443 - lr: 0.0010\n",
            "Epoch 33/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5874 - auc: 0.7422 - val_loss: 0.5860 - val_auc: 0.7446 - lr: 0.0010\n",
            "Epoch 34/1000\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.5870 - auc: 0.7423 - val_loss: 0.5856 - val_auc: 0.7447 - lr: 0.0010\n",
            "Epoch 35/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5866 - auc: 0.7428 - val_loss: 0.5852 - val_auc: 0.7450 - lr: 0.0010\n",
            "Epoch 36/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5864 - auc: 0.7426 - val_loss: 0.5849 - val_auc: 0.7450 - lr: 0.0010\n",
            "Epoch 37/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5860 - auc: 0.7430 - val_loss: 0.5846 - val_auc: 0.7451 - lr: 0.0010\n",
            "Epoch 38/1000\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.5856 - auc: 0.7431 - val_loss: 0.5843 - val_auc: 0.7454 - lr: 0.0010\n",
            "Epoch 39/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5853 - auc: 0.7432 - val_loss: 0.5841 - val_auc: 0.7455 - lr: 0.0010\n",
            "Epoch 40/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5850 - auc: 0.7433 - val_loss: 0.5837 - val_auc: 0.7457 - lr: 0.0010\n",
            "Epoch 41/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5847 - auc: 0.7435 - val_loss: 0.5834 - val_auc: 0.7458 - lr: 0.0010\n",
            "Epoch 42/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5843 - auc: 0.7437 - val_loss: 0.5835 - val_auc: 0.7458 - lr: 0.0010\n",
            "Epoch 43/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5840 - auc: 0.7438 - val_loss: 0.5836 - val_auc: 0.7461 - lr: 0.0010\n",
            "Epoch 44/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5837 - auc: 0.7440 - val_loss: 0.5824 - val_auc: 0.7461 - lr: 0.0010\n",
            "Epoch 45/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5835 - auc: 0.7441 - val_loss: 0.5821 - val_auc: 0.7461 - lr: 0.0010\n",
            "Epoch 46/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5832 - auc: 0.7442 - val_loss: 0.5818 - val_auc: 0.7462 - lr: 0.0010\n",
            "Epoch 47/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5829 - auc: 0.7444 - val_loss: 0.5816 - val_auc: 0.7463 - lr: 0.0010\n",
            "Epoch 48/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5826 - auc: 0.7445 - val_loss: 0.5813 - val_auc: 0.7465 - lr: 0.0010\n",
            "Epoch 49/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5823 - auc: 0.7445 - val_loss: 0.5810 - val_auc: 0.7466 - lr: 0.0010\n",
            "Epoch 50/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5821 - auc: 0.7447 - val_loss: 0.5809 - val_auc: 0.7467 - lr: 0.0010\n",
            "Epoch 51/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5817 - auc: 0.7449 - val_loss: 0.5805 - val_auc: 0.7468 - lr: 0.0010\n",
            "Epoch 52/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5815 - auc: 0.7450 - val_loss: 0.5804 - val_auc: 0.7469 - lr: 0.0010\n",
            "Epoch 53/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5814 - auc: 0.7450 - val_loss: 0.5800 - val_auc: 0.7468 - lr: 0.0010\n",
            "Epoch 54/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5810 - auc: 0.7454 - val_loss: 0.5797 - val_auc: 0.7470 - lr: 0.0010\n",
            "Epoch 55/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5809 - auc: 0.7452 - val_loss: 0.5795 - val_auc: 0.7469 - lr: 0.0010\n",
            "Epoch 56/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5806 - auc: 0.7454 - val_loss: 0.5793 - val_auc: 0.7470 - lr: 0.0010\n",
            "Epoch 57/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5804 - auc: 0.7453 - val_loss: 0.5791 - val_auc: 0.7465 - lr: 0.0010\n",
            "Epoch 58/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5802 - auc: 0.7454 - val_loss: 0.5788 - val_auc: 0.7469 - lr: 0.0010\n",
            "Epoch 59/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5798 - auc: 0.7459 - val_loss: 0.5786 - val_auc: 0.7471 - lr: 0.0010\n",
            "Epoch 60/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5797 - auc: 0.7458 - val_loss: 0.5791 - val_auc: 0.7473 - lr: 0.0010\n",
            "Epoch 61/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5796 - auc: 0.7457 - val_loss: 0.5789 - val_auc: 0.7472 - lr: 0.0010\n",
            "Epoch 62/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5793 - auc: 0.7458 - val_loss: 0.5780 - val_auc: 0.7472 - lr: 0.0010\n",
            "Epoch 63/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5791 - auc: 0.7458 - val_loss: 0.5778 - val_auc: 0.7471 - lr: 0.0010\n",
            "Epoch 64/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5789 - auc: 0.7459 - val_loss: 0.5778 - val_auc: 0.7473 - lr: 0.0010\n",
            "Epoch 65/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5789 - auc: 0.7459 - val_loss: 0.5784 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 66/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5786 - auc: 0.7460 - val_loss: 0.5776 - val_auc: 0.7477 - lr: 0.0010\n",
            "Epoch 67/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5784 - auc: 0.7462 - val_loss: 0.5776 - val_auc: 0.7472 - lr: 0.0010\n",
            "Epoch 68/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5782 - auc: 0.7461 - val_loss: 0.5770 - val_auc: 0.7475 - lr: 0.0010\n",
            "Epoch 69/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5780 - auc: 0.7461 - val_loss: 0.5773 - val_auc: 0.7473 - lr: 0.0010\n",
            "Epoch 70/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5778 - auc: 0.7462 - val_loss: 0.5780 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 71/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5776 - auc: 0.7463 - val_loss: 0.5763 - val_auc: 0.7472 - lr: 0.0010\n",
            "Epoch 72/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5776 - auc: 0.7462 - val_loss: 0.5761 - val_auc: 0.7471 - lr: 0.0010\n",
            "Epoch 73/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5773 - auc: 0.7463 - val_loss: 0.5760 - val_auc: 0.7473 - lr: 0.0010\n",
            "Epoch 74/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5772 - auc: 0.7464 - val_loss: 0.5759 - val_auc: 0.7472 - lr: 0.0010\n",
            "Epoch 75/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5769 - auc: 0.7465 - val_loss: 0.5762 - val_auc: 0.7471 - lr: 0.0010\n",
            "Epoch 76/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5768 - auc: 0.7466 - val_loss: 0.5754 - val_auc: 0.7472 - lr: 0.0010\n",
            "Epoch 77/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5767 - auc: 0.7466 - val_loss: 0.5762 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 78/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5767 - auc: 0.7465 - val_loss: 0.5751 - val_auc: 0.7471 - lr: 0.0010\n",
            "Epoch 79/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5764 - auc: 0.7465 - val_loss: 0.5750 - val_auc: 0.7471 - lr: 0.0010\n",
            "Epoch 80/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5762 - auc: 0.7465 - val_loss: 0.5749 - val_auc: 0.7471 - lr: 0.0010\n",
            "Epoch 81/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5762 - auc: 0.7465 - val_loss: 0.5750 - val_auc: 0.7472 - lr: 0.0010\n",
            "Epoch 82/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5759 - auc: 0.7466 - val_loss: 0.5746 - val_auc: 0.7473 - lr: 0.0010\n",
            "Epoch 83/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5758 - auc: 0.7467 - val_loss: 0.5748 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 84/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5758 - auc: 0.7466 - val_loss: 0.5743 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 85/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5756 - auc: 0.7466 - val_loss: 0.5745 - val_auc: 0.7471 - lr: 0.0010\n",
            "Epoch 86/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5755 - auc: 0.7466 - val_loss: 0.5742 - val_auc: 0.7472 - lr: 0.0010\n",
            "Epoch 87/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5754 - auc: 0.7468 - val_loss: 0.5740 - val_auc: 0.7473 - lr: 0.0010\n",
            "Epoch 88/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5753 - auc: 0.7467 - val_loss: 0.5737 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 89/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5750 - auc: 0.7467 - val_loss: 0.5736 - val_auc: 0.7473 - lr: 0.0010\n",
            "Epoch 90/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5750 - auc: 0.7467 - val_loss: 0.5736 - val_auc: 0.7472 - lr: 0.0010\n",
            "Epoch 91/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5752 - auc: 0.7466 - val_loss: 0.5737 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 92/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5747 - auc: 0.7468 - val_loss: 0.5734 - val_auc: 0.7472 - lr: 0.0010\n",
            "Epoch 93/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5746 - auc: 0.7468 - val_loss: 0.5737 - val_auc: 0.7473 - lr: 0.0010\n",
            "Epoch 94/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5746 - auc: 0.7469 - val_loss: 0.5732 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 95/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5743 - auc: 0.7467 - val_loss: 0.5732 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 96/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5744 - auc: 0.7468 - val_loss: 0.5728 - val_auc: 0.7473 - lr: 0.0010\n",
            "Epoch 97/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5742 - auc: 0.7469 - val_loss: 0.5756 - val_auc: 0.7471 - lr: 0.0010\n",
            "Epoch 98/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5742 - auc: 0.7469 - val_loss: 0.5728 - val_auc: 0.7475 - lr: 0.0010\n",
            "Epoch 99/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5740 - auc: 0.7469 - val_loss: 0.5726 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 100/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5740 - auc: 0.7468 - val_loss: 0.5724 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 101/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5739 - auc: 0.7469 - val_loss: 0.5723 - val_auc: 0.7473 - lr: 0.0010\n",
            "Epoch 102/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5736 - auc: 0.7471 - val_loss: 0.5722 - val_auc: 0.7475 - lr: 0.0010\n",
            "Epoch 103/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5736 - auc: 0.7471 - val_loss: 0.5722 - val_auc: 0.7475 - lr: 0.0010\n",
            "Epoch 104/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5735 - auc: 0.7471 - val_loss: 0.5724 - val_auc: 0.7477 - lr: 0.0010\n",
            "Epoch 105/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5736 - auc: 0.7469 - val_loss: 0.5727 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 106/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5734 - auc: 0.7471 - val_loss: 0.5721 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 107/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5735 - auc: 0.7471 - val_loss: 0.5717 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 108/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5733 - auc: 0.7469 - val_loss: 0.5717 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 109/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5730 - auc: 0.7472 - val_loss: 0.5717 - val_auc: 0.7477 - lr: 0.0010\n",
            "Epoch 110/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5731 - auc: 0.7471 - val_loss: 0.5714 - val_auc: 0.7477 - lr: 0.0010\n",
            "Epoch 111/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5729 - auc: 0.7471 - val_loss: 0.5722 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 112/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5730 - auc: 0.7470 - val_loss: 0.5712 - val_auc: 0.7477 - lr: 0.0010\n",
            "Epoch 113/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5728 - auc: 0.7472 - val_loss: 0.5714 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 114/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5730 - auc: 0.7473 - val_loss: 0.5711 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 115/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5726 - auc: 0.7471 - val_loss: 0.5718 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 116/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5726 - auc: 0.7469 - val_loss: 0.5722 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 117/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5726 - auc: 0.7471 - val_loss: 0.5708 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 118/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5726 - auc: 0.7470 - val_loss: 0.5718 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 119/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5724 - auc: 0.7471 - val_loss: 0.5707 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 120/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5723 - auc: 0.7472 - val_loss: 0.5707 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 121/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5724 - auc: 0.7469 - val_loss: 0.5706 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 122/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5722 - auc: 0.7471 - val_loss: 0.5705 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 123/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5721 - auc: 0.7471 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 124/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5721 - auc: 0.7471 - val_loss: 0.5708 - val_auc: 0.7477 - lr: 0.0010\n",
            "Epoch 125/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5721 - auc: 0.7471 - val_loss: 0.5706 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 126/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5722 - auc: 0.7472 - val_loss: 0.5702 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 127/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5721 - auc: 0.7471 - val_loss: 0.5702 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 128/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5719 - auc: 0.7472 - val_loss: 0.5702 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 129/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5719 - auc: 0.7472 - val_loss: 0.5703 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 130/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5717 - auc: 0.7472 - val_loss: 0.5700 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 131/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5717 - auc: 0.7472 - val_loss: 0.5699 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 132/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5717 - auc: 0.7472 - val_loss: 0.5699 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 133/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5717 - auc: 0.7472 - val_loss: 0.5708 - val_auc: 0.7483 - lr: 0.0010\n",
            "Epoch 134/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7472 - val_loss: 0.5697 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 135/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7474 - val_loss: 0.5700 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 136/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5717 - auc: 0.7473 - val_loss: 0.5714 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 137/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7472 - val_loss: 0.5696 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 138/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5714 - auc: 0.7473 - val_loss: 0.5701 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 139/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5713 - auc: 0.7473 - val_loss: 0.5695 - val_auc: 0.7482 - lr: 0.0010\n",
            "Epoch 140/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5715 - auc: 0.7473 - val_loss: 0.5702 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 141/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5714 - auc: 0.7473 - val_loss: 0.5697 - val_auc: 0.7484 - lr: 0.0010\n",
            "Epoch 142/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5714 - auc: 0.7472 - val_loss: 0.5723 - val_auc: 0.7477 - lr: 0.0010\n",
            "Epoch 143/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5712 - auc: 0.7473 - val_loss: 0.5695 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 144/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5711 - auc: 0.7474 - val_loss: 0.5697 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 145/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5708 - auc: 0.7474 - val_loss: 0.5692 - val_auc: 0.7483 - lr: 2.0000e-04\n",
            "Epoch 146/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5708 - auc: 0.7474 - val_loss: 0.5692 - val_auc: 0.7482 - lr: 2.0000e-04\n",
            "Epoch 147/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5708 - auc: 0.7474 - val_loss: 0.5692 - val_auc: 0.7481 - lr: 2.0000e-04\n",
            "Epoch 148/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5708 - auc: 0.7475 - val_loss: 0.5692 - val_auc: 0.7479 - lr: 2.0000e-04\n",
            "Epoch 149/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5708 - auc: 0.7474 - val_loss: 0.5692 - val_auc: 0.7479 - lr: 2.0000e-04\n",
            "Epoch 150/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5708 - auc: 0.7474 - val_loss: 0.5692 - val_auc: 0.7480 - lr: 2.0000e-04\n",
            "Epoch 151/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5707 - auc: 0.7474 - val_loss: 0.5692 - val_auc: 0.7480 - lr: 4.0000e-05\n",
            "Epoch 152/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5707 - auc: 0.7475 - val_loss: 0.5692 - val_auc: 0.7480 - lr: 4.0000e-05\n",
            "Epoch 153/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5707 - auc: 0.7475 - val_loss: 0.5691 - val_auc: 0.7480 - lr: 4.0000e-05\n",
            "Epoch 154/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5707 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 4.0000e-05\n",
            "Epoch 155/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5707 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 4.0000e-05\n",
            "Epoch 156/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5707 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 8.0000e-06\n",
            "Epoch 157/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5707 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7482 - lr: 8.0000e-06\n",
            "Epoch 158/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5707 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 8.0000e-06\n",
            "Epoch 159/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5707 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 8.0000e-06\n",
            "Epoch 160/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5707 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 8.0000e-06\n",
            "Epoch 161/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 1.6000e-06\n",
            "Epoch 162/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 1.6000e-06\n",
            "Epoch 163/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 1.6000e-06\n",
            "Epoch 164/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 1.6000e-06\n",
            "Epoch 165/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 1.6000e-06\n",
            "Epoch 166/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 3.2000e-07\n",
            "Epoch 167/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 3.2000e-07\n",
            "Epoch 168/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 3.2000e-07\n",
            "Epoch 169/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 3.2000e-07\n",
            "Epoch 170/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 3.2000e-07\n",
            "Epoch 171/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 6.4000e-08\n",
            "Epoch 172/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 6.4000e-08\n",
            "Epoch 173/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 6.4000e-08\n",
            "Epoch 174/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 6.4000e-08\n",
            "Epoch 175/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 6.4000e-08\n",
            "Epoch 176/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 1.2800e-08\n",
            "Epoch 177/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 1.2800e-08\n",
            "Epoch 178/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 1.2800e-08\n",
            "Epoch 179/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 1.2800e-08\n",
            "Epoch 180/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 1.2800e-08\n",
            "Epoch 181/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 2.5600e-09\n",
            "Epoch 182/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 2.5600e-09\n",
            "Epoch 183/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 2.5600e-09\n",
            "Epoch 184/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 2.5600e-09\n",
            "Epoch 185/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7474 - val_loss: 0.5691 - val_auc: 0.7481 - lr: 2.5600e-09\n",
            "Fold 1 NN: 0.74786\n",
            "Training fold 2\n",
            "CV 2/5\n",
            "(480000, 100) (480000,)\n",
            "(120000, 100) (120000,)\n",
            "Epoch 1/1000\n",
            "235/235 [==============================] - 4s 13ms/step - loss: 0.6931 - auc: 0.5000 - val_loss: 0.6931 - val_auc: 0.5000 - lr: 0.0010\n",
            "Epoch 2/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6931 - auc: 0.4992 - val_loss: 0.6931 - val_auc: 0.5000 - lr: 0.0010\n",
            "Epoch 3/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6917 - auc: 0.5796 - val_loss: 0.6854 - val_auc: 0.7220 - lr: 0.0010\n",
            "Epoch 4/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6554 - auc: 0.7167 - val_loss: 0.6240 - val_auc: 0.7304 - lr: 0.0010\n",
            "Epoch 5/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6118 - auc: 0.7284 - val_loss: 0.6046 - val_auc: 0.7325 - lr: 0.0010\n",
            "Epoch 6/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6032 - auc: 0.7313 - val_loss: 0.6011 - val_auc: 0.7338 - lr: 0.0010\n",
            "Epoch 7/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6013 - auc: 0.7326 - val_loss: 0.6003 - val_auc: 0.7346 - lr: 0.0010\n",
            "Epoch 8/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6002 - auc: 0.7336 - val_loss: 0.5989 - val_auc: 0.7355 - lr: 0.0010\n",
            "Epoch 9/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5992 - auc: 0.7343 - val_loss: 0.5980 - val_auc: 0.7360 - lr: 0.0010\n",
            "Epoch 10/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5985 - auc: 0.7351 - val_loss: 0.5973 - val_auc: 0.7367 - lr: 0.0010\n",
            "Epoch 11/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5977 - auc: 0.7356 - val_loss: 0.5964 - val_auc: 0.7374 - lr: 0.0010\n",
            "Epoch 12/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5970 - auc: 0.7362 - val_loss: 0.5957 - val_auc: 0.7379 - lr: 0.0010\n",
            "Epoch 13/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5963 - auc: 0.7367 - val_loss: 0.5952 - val_auc: 0.7383 - lr: 0.0010\n",
            "Epoch 14/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5957 - auc: 0.7372 - val_loss: 0.5948 - val_auc: 0.7386 - lr: 0.0010\n",
            "Epoch 15/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5951 - auc: 0.7377 - val_loss: 0.5947 - val_auc: 0.7392 - lr: 0.0010\n",
            "Epoch 16/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5945 - auc: 0.7381 - val_loss: 0.5934 - val_auc: 0.7396 - lr: 0.0010\n",
            "Epoch 17/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5939 - auc: 0.7384 - val_loss: 0.5929 - val_auc: 0.7399 - lr: 0.0010\n",
            "Epoch 18/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5934 - auc: 0.7388 - val_loss: 0.5931 - val_auc: 0.7403 - lr: 0.0010\n",
            "Epoch 19/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5929 - auc: 0.7391 - val_loss: 0.5919 - val_auc: 0.7407 - lr: 0.0010\n",
            "Epoch 20/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5924 - auc: 0.7394 - val_loss: 0.5914 - val_auc: 0.7409 - lr: 0.0010\n",
            "Epoch 21/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5919 - auc: 0.7398 - val_loss: 0.5916 - val_auc: 0.7413 - lr: 0.0010\n",
            "Epoch 22/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5914 - auc: 0.7400 - val_loss: 0.5909 - val_auc: 0.7415 - lr: 0.0010\n",
            "Epoch 23/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5910 - auc: 0.7403 - val_loss: 0.5899 - val_auc: 0.7416 - lr: 0.0010\n",
            "Epoch 24/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5906 - auc: 0.7406 - val_loss: 0.5895 - val_auc: 0.7421 - lr: 0.0010\n",
            "Epoch 25/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5901 - auc: 0.7410 - val_loss: 0.5890 - val_auc: 0.7423 - lr: 0.0010\n",
            "Epoch 26/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5897 - auc: 0.7411 - val_loss: 0.5889 - val_auc: 0.7425 - lr: 0.0010\n",
            "Epoch 27/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5893 - auc: 0.7414 - val_loss: 0.5883 - val_auc: 0.7427 - lr: 0.0010\n",
            "Epoch 28/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5889 - auc: 0.7416 - val_loss: 0.5891 - val_auc: 0.7428 - lr: 0.0010\n",
            "Epoch 29/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5885 - auc: 0.7418 - val_loss: 0.5876 - val_auc: 0.7432 - lr: 0.0010\n",
            "Epoch 30/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5881 - auc: 0.7420 - val_loss: 0.5871 - val_auc: 0.7432 - lr: 0.0010\n",
            "Epoch 31/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5877 - auc: 0.7423 - val_loss: 0.5868 - val_auc: 0.7436 - lr: 0.0010\n",
            "Epoch 32/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5873 - auc: 0.7424 - val_loss: 0.5876 - val_auc: 0.7437 - lr: 0.0010\n",
            "Epoch 33/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5870 - auc: 0.7427 - val_loss: 0.5859 - val_auc: 0.7440 - lr: 0.0010\n",
            "Epoch 34/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5866 - auc: 0.7429 - val_loss: 0.5858 - val_auc: 0.7440 - lr: 0.0010\n",
            "Epoch 35/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5863 - auc: 0.7431 - val_loss: 0.5853 - val_auc: 0.7442 - lr: 0.0010\n",
            "Epoch 36/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5859 - auc: 0.7432 - val_loss: 0.5850 - val_auc: 0.7445 - lr: 0.0010\n",
            "Epoch 37/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5855 - auc: 0.7433 - val_loss: 0.5850 - val_auc: 0.7446 - lr: 0.0010\n",
            "Epoch 38/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5853 - auc: 0.7434 - val_loss: 0.5846 - val_auc: 0.7448 - lr: 0.0010\n",
            "Epoch 39/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5850 - auc: 0.7436 - val_loss: 0.5846 - val_auc: 0.7450 - lr: 0.0010\n",
            "Epoch 40/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5846 - auc: 0.7439 - val_loss: 0.5836 - val_auc: 0.7451 - lr: 0.0010\n",
            "Epoch 41/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5843 - auc: 0.7439 - val_loss: 0.5834 - val_auc: 0.7452 - lr: 0.0010\n",
            "Epoch 42/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5840 - auc: 0.7441 - val_loss: 0.5834 - val_auc: 0.7453 - lr: 0.0010\n",
            "Epoch 43/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5838 - auc: 0.7441 - val_loss: 0.5829 - val_auc: 0.7455 - lr: 0.0010\n",
            "Epoch 44/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5834 - auc: 0.7443 - val_loss: 0.5827 - val_auc: 0.7456 - lr: 0.0010\n",
            "Epoch 45/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5833 - auc: 0.7443 - val_loss: 0.5822 - val_auc: 0.7456 - lr: 0.0010\n",
            "Epoch 46/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5828 - auc: 0.7444 - val_loss: 0.5819 - val_auc: 0.7457 - lr: 0.0010\n",
            "Epoch 47/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5829 - auc: 0.7444 - val_loss: 0.5820 - val_auc: 0.7458 - lr: 0.0010\n",
            "Epoch 48/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5823 - auc: 0.7447 - val_loss: 0.5815 - val_auc: 0.7462 - lr: 0.0010\n",
            "Epoch 49/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5820 - auc: 0.7447 - val_loss: 0.5815 - val_auc: 0.7460 - lr: 0.0010\n",
            "Epoch 50/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5818 - auc: 0.7447 - val_loss: 0.5810 - val_auc: 0.7463 - lr: 0.0010\n",
            "Epoch 51/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5815 - auc: 0.7452 - val_loss: 0.5809 - val_auc: 0.7463 - lr: 0.0010\n",
            "Epoch 52/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5813 - auc: 0.7452 - val_loss: 0.5805 - val_auc: 0.7466 - lr: 0.0010\n",
            "Epoch 53/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5811 - auc: 0.7452 - val_loss: 0.5802 - val_auc: 0.7467 - lr: 0.0010\n",
            "Epoch 54/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5809 - auc: 0.7454 - val_loss: 0.5802 - val_auc: 0.7464 - lr: 0.0010\n",
            "Epoch 55/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5806 - auc: 0.7456 - val_loss: 0.5800 - val_auc: 0.7464 - lr: 0.0010\n",
            "Epoch 56/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5803 - auc: 0.7455 - val_loss: 0.5795 - val_auc: 0.7464 - lr: 0.0010\n",
            "Epoch 57/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5801 - auc: 0.7456 - val_loss: 0.5792 - val_auc: 0.7470 - lr: 0.0010\n",
            "Epoch 58/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5800 - auc: 0.7454 - val_loss: 0.5792 - val_auc: 0.7470 - lr: 0.0010\n",
            "Epoch 59/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5798 - auc: 0.7456 - val_loss: 0.5788 - val_auc: 0.7470 - lr: 0.0010\n",
            "Epoch 60/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5794 - auc: 0.7460 - val_loss: 0.5786 - val_auc: 0.7471 - lr: 0.0010\n",
            "Epoch 61/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5794 - auc: 0.7457 - val_loss: 0.5784 - val_auc: 0.7471 - lr: 0.0010\n",
            "Epoch 62/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5793 - auc: 0.7456 - val_loss: 0.5799 - val_auc: 0.7469 - lr: 0.0010\n",
            "Epoch 63/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5791 - auc: 0.7459 - val_loss: 0.5780 - val_auc: 0.7472 - lr: 0.0010\n",
            "Epoch 64/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5786 - auc: 0.7460 - val_loss: 0.5781 - val_auc: 0.7472 - lr: 0.0010\n",
            "Epoch 65/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5785 - auc: 0.7462 - val_loss: 0.5777 - val_auc: 0.7469 - lr: 0.0010\n",
            "Epoch 66/1000\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.5783 - auc: 0.7460 - val_loss: 0.5780 - val_auc: 0.7471 - lr: 0.0010\n",
            "Epoch 67/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5781 - auc: 0.7461 - val_loss: 0.5773 - val_auc: 0.7472 - lr: 0.0010\n",
            "Epoch 68/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5779 - auc: 0.7463 - val_loss: 0.5771 - val_auc: 0.7472 - lr: 0.0010\n",
            "Epoch 69/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5779 - auc: 0.7462 - val_loss: 0.5778 - val_auc: 0.7475 - lr: 0.0010\n",
            "Epoch 70/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5777 - auc: 0.7460 - val_loss: 0.5772 - val_auc: 0.7472 - lr: 0.0010\n",
            "Epoch 71/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5775 - auc: 0.7465 - val_loss: 0.5790 - val_auc: 0.7473 - lr: 0.0010\n",
            "Epoch 72/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5772 - auc: 0.7463 - val_loss: 0.5767 - val_auc: 0.7470 - lr: 0.0010\n",
            "Epoch 73/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5771 - auc: 0.7464 - val_loss: 0.5763 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 74/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5769 - auc: 0.7465 - val_loss: 0.5766 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 75/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5770 - auc: 0.7464 - val_loss: 0.5760 - val_auc: 0.7475 - lr: 0.0010\n",
            "Epoch 76/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5767 - auc: 0.7465 - val_loss: 0.5759 - val_auc: 0.7475 - lr: 0.0010\n",
            "Epoch 77/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5766 - auc: 0.7464 - val_loss: 0.5757 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 78/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5764 - auc: 0.7464 - val_loss: 0.5764 - val_auc: 0.7475 - lr: 0.0010\n",
            "Epoch 79/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5764 - auc: 0.7465 - val_loss: 0.5760 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 80/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5761 - auc: 0.7466 - val_loss: 0.5752 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 81/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5760 - auc: 0.7465 - val_loss: 0.5751 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 82/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5758 - auc: 0.7468 - val_loss: 0.5764 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 83/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5756 - auc: 0.7466 - val_loss: 0.5752 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 84/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5755 - auc: 0.7466 - val_loss: 0.5748 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 85/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5754 - auc: 0.7466 - val_loss: 0.5761 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 86/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5753 - auc: 0.7467 - val_loss: 0.5745 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 87/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5753 - auc: 0.7467 - val_loss: 0.5751 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 88/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5751 - auc: 0.7468 - val_loss: 0.5753 - val_auc: 0.7477 - lr: 0.0010\n",
            "Epoch 89/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5751 - auc: 0.7465 - val_loss: 0.5767 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 90/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5749 - auc: 0.7468 - val_loss: 0.5742 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 91/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5749 - auc: 0.7467 - val_loss: 0.5741 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 92/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5746 - auc: 0.7468 - val_loss: 0.5738 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 93/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5745 - auc: 0.7469 - val_loss: 0.5743 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 94/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5744 - auc: 0.7468 - val_loss: 0.5735 - val_auc: 0.7477 - lr: 0.0010\n",
            "Epoch 95/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5743 - auc: 0.7469 - val_loss: 0.5735 - val_auc: 0.7477 - lr: 0.0010\n",
            "Epoch 96/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5742 - auc: 0.7468 - val_loss: 0.5734 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 97/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5740 - auc: 0.7470 - val_loss: 0.5733 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 98/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5739 - auc: 0.7470 - val_loss: 0.5741 - val_auc: 0.7475 - lr: 0.0010\n",
            "Epoch 99/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5740 - auc: 0.7467 - val_loss: 0.5731 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 100/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5737 - auc: 0.7469 - val_loss: 0.5729 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 101/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5736 - auc: 0.7468 - val_loss: 0.5742 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 102/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5739 - auc: 0.7467 - val_loss: 0.5730 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 103/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5735 - auc: 0.7468 - val_loss: 0.5727 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 104/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5734 - auc: 0.7470 - val_loss: 0.5726 - val_auc: 0.7477 - lr: 0.0010\n",
            "Epoch 105/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5734 - auc: 0.7470 - val_loss: 0.5727 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 106/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5734 - auc: 0.7468 - val_loss: 0.5724 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 107/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5733 - auc: 0.7469 - val_loss: 0.5723 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 108/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5730 - auc: 0.7471 - val_loss: 0.5722 - val_auc: 0.7477 - lr: 0.0010\n",
            "Epoch 109/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5729 - auc: 0.7471 - val_loss: 0.5734 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 110/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5729 - auc: 0.7470 - val_loss: 0.5727 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 111/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5729 - auc: 0.7470 - val_loss: 0.5720 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 112/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5727 - auc: 0.7470 - val_loss: 0.5719 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 113/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5726 - auc: 0.7470 - val_loss: 0.5720 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 114/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5726 - auc: 0.7470 - val_loss: 0.5724 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 115/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5725 - auc: 0.7470 - val_loss: 0.5717 - val_auc: 0.7475 - lr: 0.0010\n",
            "Epoch 116/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5723 - auc: 0.7471 - val_loss: 0.5716 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 117/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5723 - auc: 0.7471 - val_loss: 0.5715 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 118/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5724 - auc: 0.7470 - val_loss: 0.5715 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 119/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5723 - auc: 0.7471 - val_loss: 0.5714 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 120/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5722 - auc: 0.7471 - val_loss: 0.5715 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 121/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5721 - auc: 0.7471 - val_loss: 0.5721 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 122/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5722 - auc: 0.7470 - val_loss: 0.5727 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 123/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5720 - auc: 0.7471 - val_loss: 0.5712 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 124/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5722 - auc: 0.7471 - val_loss: 0.5711 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 125/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5720 - auc: 0.7470 - val_loss: 0.5710 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 126/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5717 - auc: 0.7473 - val_loss: 0.5711 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 127/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5718 - auc: 0.7471 - val_loss: 0.5709 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 128/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5717 - auc: 0.7471 - val_loss: 0.5712 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 129/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5718 - auc: 0.7471 - val_loss: 0.5714 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 130/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5715 - auc: 0.7472 - val_loss: 0.5711 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 131/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5713 - auc: 0.7473 - val_loss: 0.5712 - val_auc: 0.7482 - lr: 0.0010\n",
            "Epoch 132/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5714 - auc: 0.7473 - val_loss: 0.5710 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 133/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5712 - auc: 0.7473 - val_loss: 0.5706 - val_auc: 0.7480 - lr: 2.0000e-04\n",
            "Epoch 134/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5711 - auc: 0.7473 - val_loss: 0.5706 - val_auc: 0.7481 - lr: 2.0000e-04\n",
            "Epoch 135/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5710 - auc: 0.7473 - val_loss: 0.5706 - val_auc: 0.7481 - lr: 2.0000e-04\n",
            "Epoch 136/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5710 - auc: 0.7474 - val_loss: 0.5707 - val_auc: 0.7480 - lr: 2.0000e-04\n",
            "Epoch 137/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5710 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7482 - lr: 2.0000e-04\n",
            "Epoch 138/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5710 - auc: 0.7474 - val_loss: 0.5706 - val_auc: 0.7481 - lr: 2.0000e-04\n",
            "Epoch 139/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 4.0000e-05\n",
            "Epoch 140/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7482 - lr: 4.0000e-05\n",
            "Epoch 141/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5706 - val_auc: 0.7480 - lr: 4.0000e-05\n",
            "Epoch 142/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7480 - lr: 4.0000e-05\n",
            "Epoch 143/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 4.0000e-05\n",
            "Epoch 144/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 8.0000e-06\n",
            "Epoch 145/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7480 - lr: 8.0000e-06\n",
            "Epoch 146/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7480 - lr: 8.0000e-06\n",
            "Epoch 147/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 8.0000e-06\n",
            "Epoch 148/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 8.0000e-06\n",
            "Epoch 149/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 1.6000e-06\n",
            "Epoch 150/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 1.6000e-06\n",
            "Epoch 151/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 1.6000e-06\n",
            "Epoch 152/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 1.6000e-06\n",
            "Epoch 153/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 1.6000e-06\n",
            "Epoch 154/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 3.2000e-07\n",
            "Epoch 155/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 3.2000e-07\n",
            "Epoch 156/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 3.2000e-07\n",
            "Epoch 157/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 3.2000e-07\n",
            "Epoch 158/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 3.2000e-07\n",
            "Epoch 159/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 6.4000e-08\n",
            "Epoch 160/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 6.4000e-08\n",
            "Epoch 161/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 6.4000e-08\n",
            "Epoch 162/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 6.4000e-08\n",
            "Epoch 163/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 6.4000e-08\n",
            "Epoch 164/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 1.2800e-08\n",
            "Epoch 165/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 1.2800e-08\n",
            "Epoch 166/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 1.2800e-08\n",
            "Epoch 167/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 1.2800e-08\n",
            "Epoch 168/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5705 - val_auc: 0.7481 - lr: 1.2800e-08\n",
            "Fold 2 NN: 0.74799\n",
            "Training fold 3\n",
            "CV 3/5\n",
            "(480000, 100) (480000,)\n",
            "(120000, 100) (120000,)\n",
            "Epoch 1/1000\n",
            "235/235 [==============================] - 4s 13ms/step - loss: 0.6931 - auc: 0.4998 - val_loss: 0.6931 - val_auc: 0.5000 - lr: 0.0010\n",
            "Epoch 2/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6931 - auc: 0.5007 - val_loss: 0.6931 - val_auc: 0.5000 - lr: 0.0010\n",
            "Epoch 3/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6926 - auc: 0.5549 - val_loss: 0.6896 - val_auc: 0.7043 - lr: 0.0010\n",
            "Epoch 4/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.6627 - auc: 0.7126 - val_loss: 0.6299 - val_auc: 0.7275 - lr: 0.0010\n",
            "Epoch 5/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.6140 - auc: 0.7275 - val_loss: 0.6066 - val_auc: 0.7296 - lr: 0.0010\n",
            "Epoch 6/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.6036 - auc: 0.7314 - val_loss: 0.6033 - val_auc: 0.7310 - lr: 0.0010\n",
            "Epoch 7/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.6015 - auc: 0.7327 - val_loss: 0.6024 - val_auc: 0.7318 - lr: 0.0010\n",
            "Epoch 8/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6003 - auc: 0.7336 - val_loss: 0.6013 - val_auc: 0.7327 - lr: 0.0010\n",
            "Epoch 9/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5994 - auc: 0.7344 - val_loss: 0.6003 - val_auc: 0.7334 - lr: 0.0010\n",
            "Epoch 10/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5985 - auc: 0.7352 - val_loss: 0.5995 - val_auc: 0.7340 - lr: 0.0010\n",
            "Epoch 11/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5977 - auc: 0.7357 - val_loss: 0.5989 - val_auc: 0.7346 - lr: 0.0010\n",
            "Epoch 12/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5970 - auc: 0.7363 - val_loss: 0.5980 - val_auc: 0.7351 - lr: 0.0010\n",
            "Epoch 13/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5963 - auc: 0.7368 - val_loss: 0.5973 - val_auc: 0.7356 - lr: 0.0010\n",
            "Epoch 14/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5957 - auc: 0.7374 - val_loss: 0.5967 - val_auc: 0.7361 - lr: 0.0010\n",
            "Epoch 15/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5950 - auc: 0.7379 - val_loss: 0.5964 - val_auc: 0.7365 - lr: 0.0010\n",
            "Epoch 16/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5944 - auc: 0.7383 - val_loss: 0.5958 - val_auc: 0.7370 - lr: 0.0010\n",
            "Epoch 17/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5938 - auc: 0.7387 - val_loss: 0.5951 - val_auc: 0.7374 - lr: 0.0010\n",
            "Epoch 18/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5932 - auc: 0.7391 - val_loss: 0.5946 - val_auc: 0.7377 - lr: 0.0010\n",
            "Epoch 19/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5926 - auc: 0.7395 - val_loss: 0.5944 - val_auc: 0.7381 - lr: 0.0010\n",
            "Epoch 20/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5923 - auc: 0.7398 - val_loss: 0.5934 - val_auc: 0.7384 - lr: 0.0010\n",
            "Epoch 21/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5917 - auc: 0.7401 - val_loss: 0.5930 - val_auc: 0.7386 - lr: 0.0010\n",
            "Epoch 22/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5913 - auc: 0.7404 - val_loss: 0.5925 - val_auc: 0.7390 - lr: 0.0010\n",
            "Epoch 23/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5907 - auc: 0.7408 - val_loss: 0.5922 - val_auc: 0.7392 - lr: 0.0010\n",
            "Epoch 24/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5903 - auc: 0.7410 - val_loss: 0.5916 - val_auc: 0.7395 - lr: 0.0010\n",
            "Epoch 25/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5898 - auc: 0.7414 - val_loss: 0.5912 - val_auc: 0.7398 - lr: 0.0010\n",
            "Epoch 26/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5893 - auc: 0.7418 - val_loss: 0.5908 - val_auc: 0.7401 - lr: 0.0010\n",
            "Epoch 27/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5890 - auc: 0.7418 - val_loss: 0.5905 - val_auc: 0.7403 - lr: 0.0010\n",
            "Epoch 28/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5885 - auc: 0.7421 - val_loss: 0.5904 - val_auc: 0.7404 - lr: 0.0010\n",
            "Epoch 29/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5881 - auc: 0.7423 - val_loss: 0.5898 - val_auc: 0.7407 - lr: 0.0010\n",
            "Epoch 30/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5877 - auc: 0.7425 - val_loss: 0.5891 - val_auc: 0.7410 - lr: 0.0010\n",
            "Epoch 31/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5874 - auc: 0.7428 - val_loss: 0.5892 - val_auc: 0.7412 - lr: 0.0010\n",
            "Epoch 32/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5869 - auc: 0.7430 - val_loss: 0.5884 - val_auc: 0.7415 - lr: 0.0010\n",
            "Epoch 33/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5865 - auc: 0.7432 - val_loss: 0.5880 - val_auc: 0.7416 - lr: 0.0010\n",
            "Epoch 34/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5863 - auc: 0.7433 - val_loss: 0.5877 - val_auc: 0.7417 - lr: 0.0010\n",
            "Epoch 35/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5859 - auc: 0.7435 - val_loss: 0.5874 - val_auc: 0.7419 - lr: 0.0010\n",
            "Epoch 36/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5855 - auc: 0.7436 - val_loss: 0.5883 - val_auc: 0.7419 - lr: 0.0010\n",
            "Epoch 37/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5852 - auc: 0.7439 - val_loss: 0.5867 - val_auc: 0.7423 - lr: 0.0010\n",
            "Epoch 38/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5848 - auc: 0.7441 - val_loss: 0.5867 - val_auc: 0.7424 - lr: 0.0010\n",
            "Epoch 39/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5846 - auc: 0.7442 - val_loss: 0.5861 - val_auc: 0.7425 - lr: 0.0010\n",
            "Epoch 40/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5842 - auc: 0.7444 - val_loss: 0.5863 - val_auc: 0.7428 - lr: 0.0010\n",
            "Epoch 41/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5838 - auc: 0.7445 - val_loss: 0.5854 - val_auc: 0.7426 - lr: 0.0010\n",
            "Epoch 42/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5836 - auc: 0.7447 - val_loss: 0.5853 - val_auc: 0.7429 - lr: 0.0010\n",
            "Epoch 43/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5833 - auc: 0.7449 - val_loss: 0.5848 - val_auc: 0.7426 - lr: 0.0010\n",
            "Epoch 44/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5831 - auc: 0.7448 - val_loss: 0.5849 - val_auc: 0.7429 - lr: 0.0010\n",
            "Epoch 45/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5828 - auc: 0.7450 - val_loss: 0.5844 - val_auc: 0.7430 - lr: 0.0010\n",
            "Epoch 46/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5824 - auc: 0.7451 - val_loss: 0.5839 - val_auc: 0.7433 - lr: 0.0010\n",
            "Epoch 47/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5821 - auc: 0.7452 - val_loss: 0.5836 - val_auc: 0.7434 - lr: 0.0010\n",
            "Epoch 48/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5820 - auc: 0.7452 - val_loss: 0.5836 - val_auc: 0.7436 - lr: 0.0010\n",
            "Epoch 49/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5816 - auc: 0.7453 - val_loss: 0.5832 - val_auc: 0.7437 - lr: 0.0010\n",
            "Epoch 50/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5813 - auc: 0.7458 - val_loss: 0.5828 - val_auc: 0.7437 - lr: 0.0010\n",
            "Epoch 51/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5810 - auc: 0.7456 - val_loss: 0.5830 - val_auc: 0.7437 - lr: 0.0010\n",
            "Epoch 52/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5808 - auc: 0.7457 - val_loss: 0.5824 - val_auc: 0.7439 - lr: 0.0010\n",
            "Epoch 53/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5805 - auc: 0.7457 - val_loss: 0.5831 - val_auc: 0.7440 - lr: 0.0010\n",
            "Epoch 54/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5802 - auc: 0.7459 - val_loss: 0.5819 - val_auc: 0.7440 - lr: 0.0010\n",
            "Epoch 55/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5801 - auc: 0.7461 - val_loss: 0.5823 - val_auc: 0.7444 - lr: 0.0010\n",
            "Epoch 56/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5798 - auc: 0.7461 - val_loss: 0.5813 - val_auc: 0.7443 - lr: 0.0010\n",
            "Epoch 57/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5796 - auc: 0.7462 - val_loss: 0.5812 - val_auc: 0.7445 - lr: 0.0010\n",
            "Epoch 58/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5794 - auc: 0.7462 - val_loss: 0.5808 - val_auc: 0.7444 - lr: 0.0010\n",
            "Epoch 59/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5791 - auc: 0.7461 - val_loss: 0.5809 - val_auc: 0.7449 - lr: 0.0010\n",
            "Epoch 60/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5789 - auc: 0.7463 - val_loss: 0.5804 - val_auc: 0.7444 - lr: 0.0010\n",
            "Epoch 61/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5789 - auc: 0.7465 - val_loss: 0.5802 - val_auc: 0.7446 - lr: 0.0010\n",
            "Epoch 62/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5786 - auc: 0.7464 - val_loss: 0.5799 - val_auc: 0.7447 - lr: 0.0010\n",
            "Epoch 63/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5783 - auc: 0.7467 - val_loss: 0.5801 - val_auc: 0.7446 - lr: 0.0010\n",
            "Epoch 64/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5781 - auc: 0.7467 - val_loss: 0.5798 - val_auc: 0.7449 - lr: 0.0010\n",
            "Epoch 65/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5779 - auc: 0.7465 - val_loss: 0.5800 - val_auc: 0.7450 - lr: 0.0010\n",
            "Epoch 66/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5779 - auc: 0.7465 - val_loss: 0.5797 - val_auc: 0.7448 - lr: 0.0010\n",
            "Epoch 67/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5775 - auc: 0.7465 - val_loss: 0.5790 - val_auc: 0.7452 - lr: 0.0010\n",
            "Epoch 68/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5773 - auc: 0.7467 - val_loss: 0.5789 - val_auc: 0.7452 - lr: 0.0010\n",
            "Epoch 69/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5772 - auc: 0.7470 - val_loss: 0.5786 - val_auc: 0.7451 - lr: 0.0010\n",
            "Epoch 70/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5769 - auc: 0.7469 - val_loss: 0.5785 - val_auc: 0.7454 - lr: 0.0010\n",
            "Epoch 71/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5767 - auc: 0.7467 - val_loss: 0.5783 - val_auc: 0.7452 - lr: 0.0010\n",
            "Epoch 72/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5767 - auc: 0.7465 - val_loss: 0.5781 - val_auc: 0.7454 - lr: 0.0010\n",
            "Epoch 73/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5766 - auc: 0.7467 - val_loss: 0.5780 - val_auc: 0.7455 - lr: 0.0010\n",
            "Epoch 74/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5763 - auc: 0.7469 - val_loss: 0.5777 - val_auc: 0.7456 - lr: 0.0010\n",
            "Epoch 75/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5761 - auc: 0.7470 - val_loss: 0.5776 - val_auc: 0.7457 - lr: 0.0010\n",
            "Epoch 76/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5760 - auc: 0.7469 - val_loss: 0.5778 - val_auc: 0.7456 - lr: 0.0010\n",
            "Epoch 77/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5759 - auc: 0.7469 - val_loss: 0.5773 - val_auc: 0.7456 - lr: 0.0010\n",
            "Epoch 78/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5757 - auc: 0.7470 - val_loss: 0.5774 - val_auc: 0.7457 - lr: 0.0010\n",
            "Epoch 79/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5756 - auc: 0.7470 - val_loss: 0.5770 - val_auc: 0.7458 - lr: 0.0010\n",
            "Epoch 80/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5753 - auc: 0.7468 - val_loss: 0.5769 - val_auc: 0.7459 - lr: 0.0010\n",
            "Epoch 81/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5752 - auc: 0.7471 - val_loss: 0.5770 - val_auc: 0.7456 - lr: 0.0010\n",
            "Epoch 82/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5751 - auc: 0.7469 - val_loss: 0.5767 - val_auc: 0.7458 - lr: 0.0010\n",
            "Epoch 83/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5751 - auc: 0.7471 - val_loss: 0.5764 - val_auc: 0.7459 - lr: 0.0010\n",
            "Epoch 84/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5750 - auc: 0.7467 - val_loss: 0.5764 - val_auc: 0.7458 - lr: 0.0010\n",
            "Epoch 85/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5747 - auc: 0.7471 - val_loss: 0.5764 - val_auc: 0.7462 - lr: 0.0010\n",
            "Epoch 86/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5747 - auc: 0.7469 - val_loss: 0.5762 - val_auc: 0.7460 - lr: 0.0010\n",
            "Epoch 87/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5746 - auc: 0.7471 - val_loss: 0.5759 - val_auc: 0.7461 - lr: 0.0010\n",
            "Epoch 88/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5743 - auc: 0.7471 - val_loss: 0.5760 - val_auc: 0.7461 - lr: 0.0010\n",
            "Epoch 89/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5742 - auc: 0.7472 - val_loss: 0.5760 - val_auc: 0.7462 - lr: 0.0010\n",
            "Epoch 90/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5742 - auc: 0.7471 - val_loss: 0.5760 - val_auc: 0.7461 - lr: 0.0010\n",
            "Epoch 91/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5742 - auc: 0.7471 - val_loss: 0.5758 - val_auc: 0.7463 - lr: 0.0010\n",
            "Epoch 92/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5739 - auc: 0.7471 - val_loss: 0.5754 - val_auc: 0.7461 - lr: 0.0010\n",
            "Epoch 93/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5739 - auc: 0.7471 - val_loss: 0.5753 - val_auc: 0.7462 - lr: 0.0010\n",
            "Epoch 94/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5738 - auc: 0.7471 - val_loss: 0.5751 - val_auc: 0.7462 - lr: 0.0010\n",
            "Epoch 95/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5735 - auc: 0.7471 - val_loss: 0.5758 - val_auc: 0.7463 - lr: 0.0010\n",
            "Epoch 96/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5735 - auc: 0.7472 - val_loss: 0.5750 - val_auc: 0.7464 - lr: 0.0010\n",
            "Epoch 97/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5734 - auc: 0.7471 - val_loss: 0.5749 - val_auc: 0.7462 - lr: 0.0010\n",
            "Epoch 98/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5732 - auc: 0.7472 - val_loss: 0.5747 - val_auc: 0.7463 - lr: 0.0010\n",
            "Epoch 99/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5730 - auc: 0.7473 - val_loss: 0.5749 - val_auc: 0.7465 - lr: 0.0010\n",
            "Epoch 100/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5731 - auc: 0.7473 - val_loss: 0.5756 - val_auc: 0.7465 - lr: 0.0010\n",
            "Epoch 101/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5730 - auc: 0.7472 - val_loss: 0.5745 - val_auc: 0.7466 - lr: 0.0010\n",
            "Epoch 102/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5728 - auc: 0.7472 - val_loss: 0.5743 - val_auc: 0.7464 - lr: 0.0010\n",
            "Epoch 103/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5730 - auc: 0.7473 - val_loss: 0.5756 - val_auc: 0.7465 - lr: 0.0010\n",
            "Epoch 104/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5727 - auc: 0.7471 - val_loss: 0.5745 - val_auc: 0.7465 - lr: 0.0010\n",
            "Epoch 105/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5727 - auc: 0.7474 - val_loss: 0.5743 - val_auc: 0.7466 - lr: 0.0010\n",
            "Epoch 106/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5726 - auc: 0.7471 - val_loss: 0.5742 - val_auc: 0.7465 - lr: 0.0010\n",
            "Epoch 107/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5724 - auc: 0.7473 - val_loss: 0.5742 - val_auc: 0.7465 - lr: 0.0010\n",
            "Epoch 108/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5724 - auc: 0.7473 - val_loss: 0.5738 - val_auc: 0.7467 - lr: 0.0010\n",
            "Epoch 109/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5724 - auc: 0.7473 - val_loss: 0.5743 - val_auc: 0.7467 - lr: 0.0010\n",
            "Epoch 110/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5722 - auc: 0.7473 - val_loss: 0.5740 - val_auc: 0.7468 - lr: 0.0010\n",
            "Epoch 111/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5720 - auc: 0.7473 - val_loss: 0.5738 - val_auc: 0.7465 - lr: 0.0010\n",
            "Epoch 112/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5720 - auc: 0.7471 - val_loss: 0.5736 - val_auc: 0.7464 - lr: 0.0010\n",
            "Epoch 113/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5722 - auc: 0.7475 - val_loss: 0.5755 - val_auc: 0.7466 - lr: 0.0010\n",
            "Epoch 114/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5719 - auc: 0.7475 - val_loss: 0.5738 - val_auc: 0.7465 - lr: 0.0010\n",
            "Epoch 115/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5717 - auc: 0.7473 - val_loss: 0.5742 - val_auc: 0.7469 - lr: 0.0010\n",
            "Epoch 116/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5718 - auc: 0.7473 - val_loss: 0.5733 - val_auc: 0.7468 - lr: 0.0010\n",
            "Epoch 117/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5717 - auc: 0.7474 - val_loss: 0.5732 - val_auc: 0.7464 - lr: 0.0010\n",
            "Epoch 118/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5719 - auc: 0.7472 - val_loss: 0.5740 - val_auc: 0.7467 - lr: 0.0010\n",
            "Epoch 119/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7474 - val_loss: 0.5731 - val_auc: 0.7465 - lr: 0.0010\n",
            "Epoch 120/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5715 - auc: 0.7473 - val_loss: 0.5731 - val_auc: 0.7465 - lr: 0.0010\n",
            "Epoch 121/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5718 - auc: 0.7473 - val_loss: 0.5737 - val_auc: 0.7466 - lr: 0.0010\n",
            "Epoch 122/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5714 - auc: 0.7475 - val_loss: 0.5729 - val_auc: 0.7466 - lr: 0.0010\n",
            "Epoch 123/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5715 - auc: 0.7475 - val_loss: 0.5731 - val_auc: 0.7467 - lr: 0.0010\n",
            "Epoch 124/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5714 - auc: 0.7475 - val_loss: 0.5728 - val_auc: 0.7467 - lr: 0.0010\n",
            "Epoch 125/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5710 - auc: 0.7474 - val_loss: 0.5734 - val_auc: 0.7468 - lr: 0.0010\n",
            "Epoch 126/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5711 - auc: 0.7475 - val_loss: 0.5741 - val_auc: 0.7467 - lr: 0.0010\n",
            "Epoch 127/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5711 - auc: 0.7475 - val_loss: 0.5741 - val_auc: 0.7468 - lr: 0.0010\n",
            "Epoch 128/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5708 - auc: 0.7476 - val_loss: 0.5727 - val_auc: 0.7469 - lr: 2.0000e-04\n",
            "Epoch 129/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5707 - auc: 0.7475 - val_loss: 0.5727 - val_auc: 0.7467 - lr: 2.0000e-04\n",
            "Epoch 130/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5707 - auc: 0.7476 - val_loss: 0.5726 - val_auc: 0.7467 - lr: 2.0000e-04\n",
            "Epoch 131/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5707 - auc: 0.7476 - val_loss: 0.5735 - val_auc: 0.7466 - lr: 2.0000e-04\n",
            "Epoch 132/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5707 - auc: 0.7476 - val_loss: 0.5726 - val_auc: 0.7468 - lr: 2.0000e-04\n",
            "Epoch 133/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5707 - auc: 0.7476 - val_loss: 0.5727 - val_auc: 0.7468 - lr: 2.0000e-04\n",
            "Epoch 134/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5706 - auc: 0.7476 - val_loss: 0.5726 - val_auc: 0.7467 - lr: 4.0000e-05\n",
            "Epoch 135/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7476 - val_loss: 0.5726 - val_auc: 0.7468 - lr: 4.0000e-05\n",
            "Epoch 136/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7476 - val_loss: 0.5726 - val_auc: 0.7468 - lr: 4.0000e-05\n",
            "Epoch 137/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7476 - val_loss: 0.5726 - val_auc: 0.7467 - lr: 4.0000e-05\n",
            "Epoch 138/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7476 - val_loss: 0.5726 - val_auc: 0.7468 - lr: 4.0000e-05\n",
            "Epoch 139/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5706 - auc: 0.7476 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 4.0000e-05\n",
            "Epoch 140/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5706 - auc: 0.7476 - val_loss: 0.5726 - val_auc: 0.7469 - lr: 4.0000e-05\n",
            "Epoch 141/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5706 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 4.0000e-05\n",
            "Epoch 142/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 4.0000e-05\n",
            "Epoch 143/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5706 - auc: 0.7476 - val_loss: 0.5726 - val_auc: 0.7468 - lr: 4.0000e-05\n",
            "Epoch 144/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5706 - auc: 0.7476 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 8.0000e-06\n",
            "Epoch 145/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5706 - auc: 0.7476 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 8.0000e-06\n",
            "Epoch 146/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5706 - auc: 0.7476 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 8.0000e-06\n",
            "Epoch 147/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5706 - auc: 0.7476 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 8.0000e-06\n",
            "Epoch 148/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5706 - auc: 0.7476 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 8.0000e-06\n",
            "Epoch 149/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7476 - val_loss: 0.5725 - val_auc: 0.7467 - lr: 1.6000e-06\n",
            "Epoch 150/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5706 - auc: 0.7476 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 1.6000e-06\n",
            "Epoch 151/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7476 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 1.6000e-06\n",
            "Epoch 152/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7476 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 1.6000e-06\n",
            "Epoch 153/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 1.6000e-06\n",
            "Epoch 154/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 3.2000e-07\n",
            "Epoch 155/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 3.2000e-07\n",
            "Epoch 156/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 3.2000e-07\n",
            "Epoch 157/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 3.2000e-07\n",
            "Epoch 158/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 3.2000e-07\n",
            "Epoch 159/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 6.4000e-08\n",
            "Epoch 160/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 6.4000e-08\n",
            "Epoch 161/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 6.4000e-08\n",
            "Epoch 162/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 6.4000e-08\n",
            "Epoch 163/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 6.4000e-08\n",
            "Epoch 164/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 1.2800e-08\n",
            "Epoch 165/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 1.2800e-08\n",
            "Epoch 166/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 1.2800e-08\n",
            "Epoch 167/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 1.2800e-08\n",
            "Epoch 168/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 1.2800e-08\n",
            "Epoch 169/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 2.5600e-09\n",
            "Epoch 170/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 2.5600e-09\n",
            "Epoch 171/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 2.5600e-09\n",
            "Epoch 172/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 2.5600e-09\n",
            "Epoch 173/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 2.5600e-09\n",
            "Epoch 174/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 5.1200e-10\n",
            "Epoch 175/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 5.1200e-10\n",
            "Epoch 176/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 5.1200e-10\n",
            "Epoch 177/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 5.1200e-10\n",
            "Epoch 178/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5705 - auc: 0.7475 - val_loss: 0.5725 - val_auc: 0.7468 - lr: 5.1200e-10\n",
            "Fold 3 NN: 0.74659\n",
            "Training fold 4\n",
            "CV 4/5\n",
            "(480000, 100) (480000,)\n",
            "(120000, 100) (120000,)\n",
            "Epoch 1/1000\n",
            "235/235 [==============================] - 4s 13ms/step - loss: 0.6931 - auc: 0.4994 - val_loss: 0.6931 - val_auc: 0.5000 - lr: 0.0010\n",
            "Epoch 2/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6931 - auc: 0.5001 - val_loss: 0.6931 - val_auc: 0.5000 - lr: 0.0010\n",
            "Epoch 3/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.6929 - auc: 0.5217 - val_loss: 0.6918 - val_auc: 0.6332 - lr: 0.0010\n",
            "Epoch 4/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.6712 - auc: 0.7068 - val_loss: 0.6370 - val_auc: 0.7276 - lr: 0.0010\n",
            "Epoch 5/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6168 - auc: 0.7265 - val_loss: 0.6067 - val_auc: 0.7300 - lr: 0.0010\n",
            "Epoch 6/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.6040 - auc: 0.7310 - val_loss: 0.6039 - val_auc: 0.7314 - lr: 0.0010\n",
            "Epoch 7/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6019 - auc: 0.7324 - val_loss: 0.6016 - val_auc: 0.7324 - lr: 0.0010\n",
            "Epoch 8/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.6008 - auc: 0.7334 - val_loss: 0.6008 - val_auc: 0.7330 - lr: 0.0010\n",
            "Epoch 9/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5998 - auc: 0.7341 - val_loss: 0.6011 - val_auc: 0.7338 - lr: 0.0010\n",
            "Epoch 10/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5990 - auc: 0.7347 - val_loss: 0.5992 - val_auc: 0.7344 - lr: 0.0010\n",
            "Epoch 11/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5982 - auc: 0.7354 - val_loss: 0.5984 - val_auc: 0.7351 - lr: 0.0010\n",
            "Epoch 12/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5976 - auc: 0.7359 - val_loss: 0.5977 - val_auc: 0.7354 - lr: 0.0010\n",
            "Epoch 13/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5968 - auc: 0.7364 - val_loss: 0.5970 - val_auc: 0.7360 - lr: 0.0010\n",
            "Epoch 14/1000\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.5962 - auc: 0.7370 - val_loss: 0.5965 - val_auc: 0.7366 - lr: 0.0010\n",
            "Epoch 15/1000\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.5955 - auc: 0.7374 - val_loss: 0.5958 - val_auc: 0.7371 - lr: 0.0010\n",
            "Epoch 16/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5950 - auc: 0.7379 - val_loss: 0.5952 - val_auc: 0.7375 - lr: 0.0010\n",
            "Epoch 17/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5944 - auc: 0.7382 - val_loss: 0.5947 - val_auc: 0.7379 - lr: 0.0010\n",
            "Epoch 18/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5938 - auc: 0.7387 - val_loss: 0.5941 - val_auc: 0.7382 - lr: 0.0010\n",
            "Epoch 19/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5934 - auc: 0.7389 - val_loss: 0.5935 - val_auc: 0.7386 - lr: 0.0010\n",
            "Epoch 20/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5927 - auc: 0.7394 - val_loss: 0.5930 - val_auc: 0.7389 - lr: 0.0010\n",
            "Epoch 21/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5924 - auc: 0.7397 - val_loss: 0.5925 - val_auc: 0.7393 - lr: 0.0010\n",
            "Epoch 22/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5918 - auc: 0.7400 - val_loss: 0.5924 - val_auc: 0.7394 - lr: 0.0010\n",
            "Epoch 23/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5913 - auc: 0.7403 - val_loss: 0.5916 - val_auc: 0.7399 - lr: 0.0010\n",
            "Epoch 24/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5908 - auc: 0.7407 - val_loss: 0.5911 - val_auc: 0.7401 - lr: 0.0010\n",
            "Epoch 25/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5905 - auc: 0.7408 - val_loss: 0.5914 - val_auc: 0.7405 - lr: 0.0010\n",
            "Epoch 26/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5900 - auc: 0.7411 - val_loss: 0.5902 - val_auc: 0.7407 - lr: 0.0010\n",
            "Epoch 27/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5895 - auc: 0.7415 - val_loss: 0.5902 - val_auc: 0.7410 - lr: 0.0010\n",
            "Epoch 28/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5891 - auc: 0.7417 - val_loss: 0.5895 - val_auc: 0.7411 - lr: 0.0010\n",
            "Epoch 29/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5886 - auc: 0.7419 - val_loss: 0.5892 - val_auc: 0.7415 - lr: 0.0010\n",
            "Epoch 30/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5882 - auc: 0.7422 - val_loss: 0.5888 - val_auc: 0.7416 - lr: 0.0010\n",
            "Epoch 31/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5879 - auc: 0.7424 - val_loss: 0.5882 - val_auc: 0.7417 - lr: 0.0010\n",
            "Epoch 32/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5875 - auc: 0.7426 - val_loss: 0.5878 - val_auc: 0.7421 - lr: 0.0010\n",
            "Epoch 33/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5871 - auc: 0.7428 - val_loss: 0.5875 - val_auc: 0.7424 - lr: 0.0010\n",
            "Epoch 34/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5867 - auc: 0.7430 - val_loss: 0.5872 - val_auc: 0.7425 - lr: 0.0010\n",
            "Epoch 35/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5863 - auc: 0.7431 - val_loss: 0.5870 - val_auc: 0.7427 - lr: 0.0010\n",
            "Epoch 36/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5861 - auc: 0.7433 - val_loss: 0.5863 - val_auc: 0.7427 - lr: 0.0010\n",
            "Epoch 37/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5857 - auc: 0.7435 - val_loss: 0.5862 - val_auc: 0.7428 - lr: 0.0010\n",
            "Epoch 38/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5853 - auc: 0.7436 - val_loss: 0.5861 - val_auc: 0.7432 - lr: 0.0010\n",
            "Epoch 39/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5851 - auc: 0.7437 - val_loss: 0.5853 - val_auc: 0.7431 - lr: 0.0010\n",
            "Epoch 40/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5849 - auc: 0.7438 - val_loss: 0.5850 - val_auc: 0.7431 - lr: 0.0010\n",
            "Epoch 41/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5843 - auc: 0.7441 - val_loss: 0.5848 - val_auc: 0.7436 - lr: 0.0010\n",
            "Epoch 42/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5841 - auc: 0.7443 - val_loss: 0.5854 - val_auc: 0.7436 - lr: 0.0010\n",
            "Epoch 43/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5838 - auc: 0.7444 - val_loss: 0.5842 - val_auc: 0.7433 - lr: 0.0010\n",
            "Epoch 44/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5836 - auc: 0.7445 - val_loss: 0.5839 - val_auc: 0.7439 - lr: 0.0010\n",
            "Epoch 45/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5833 - auc: 0.7446 - val_loss: 0.5835 - val_auc: 0.7438 - lr: 0.0010\n",
            "Epoch 46/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5829 - auc: 0.7447 - val_loss: 0.5833 - val_auc: 0.7438 - lr: 0.0010\n",
            "Epoch 47/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5826 - auc: 0.7449 - val_loss: 0.5830 - val_auc: 0.7440 - lr: 0.0010\n",
            "Epoch 48/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5823 - auc: 0.7450 - val_loss: 0.5828 - val_auc: 0.7443 - lr: 0.0010\n",
            "Epoch 49/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5822 - auc: 0.7449 - val_loss: 0.5825 - val_auc: 0.7442 - lr: 0.0010\n",
            "Epoch 50/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5818 - auc: 0.7449 - val_loss: 0.5824 - val_auc: 0.7443 - lr: 0.0010\n",
            "Epoch 51/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5816 - auc: 0.7451 - val_loss: 0.5819 - val_auc: 0.7446 - lr: 0.0010\n",
            "Epoch 52/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5813 - auc: 0.7454 - val_loss: 0.5820 - val_auc: 0.7447 - lr: 0.0010\n",
            "Epoch 53/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5811 - auc: 0.7454 - val_loss: 0.5820 - val_auc: 0.7447 - lr: 0.0010\n",
            "Epoch 54/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5809 - auc: 0.7456 - val_loss: 0.5812 - val_auc: 0.7443 - lr: 0.0010\n",
            "Epoch 55/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5806 - auc: 0.7456 - val_loss: 0.5815 - val_auc: 0.7450 - lr: 0.0010\n",
            "Epoch 56/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5805 - auc: 0.7454 - val_loss: 0.5808 - val_auc: 0.7445 - lr: 0.0010\n",
            "Epoch 57/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5804 - auc: 0.7456 - val_loss: 0.5807 - val_auc: 0.7450 - lr: 0.0010\n",
            "Epoch 58/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5800 - auc: 0.7458 - val_loss: 0.5803 - val_auc: 0.7449 - lr: 0.0010\n",
            "Epoch 59/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5797 - auc: 0.7461 - val_loss: 0.5801 - val_auc: 0.7452 - lr: 0.0010\n",
            "Epoch 60/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5794 - auc: 0.7461 - val_loss: 0.5803 - val_auc: 0.7451 - lr: 0.0010\n",
            "Epoch 61/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5792 - auc: 0.7462 - val_loss: 0.5799 - val_auc: 0.7452 - lr: 0.0010\n",
            "Epoch 62/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5791 - auc: 0.7464 - val_loss: 0.5794 - val_auc: 0.7452 - lr: 0.0010\n",
            "Epoch 63/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5789 - auc: 0.7464 - val_loss: 0.5794 - val_auc: 0.7450 - lr: 0.0010\n",
            "Epoch 64/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5787 - auc: 0.7463 - val_loss: 0.5791 - val_auc: 0.7451 - lr: 0.0010\n",
            "Epoch 65/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5785 - auc: 0.7466 - val_loss: 0.5790 - val_auc: 0.7452 - lr: 0.0010\n",
            "Epoch 66/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5783 - auc: 0.7464 - val_loss: 0.5794 - val_auc: 0.7450 - lr: 0.0010\n",
            "Epoch 67/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5781 - auc: 0.7466 - val_loss: 0.5793 - val_auc: 0.7451 - lr: 0.0010\n",
            "Epoch 68/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5779 - auc: 0.7469 - val_loss: 0.5800 - val_auc: 0.7451 - lr: 0.0010\n",
            "Epoch 69/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5778 - auc: 0.7466 - val_loss: 0.5782 - val_auc: 0.7451 - lr: 0.0010\n",
            "Epoch 70/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5775 - auc: 0.7465 - val_loss: 0.5785 - val_auc: 0.7453 - lr: 0.0010\n",
            "Epoch 71/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5776 - auc: 0.7468 - val_loss: 0.5779 - val_auc: 0.7453 - lr: 0.0010\n",
            "Epoch 72/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5772 - auc: 0.7468 - val_loss: 0.5783 - val_auc: 0.7453 - lr: 0.0010\n",
            "Epoch 73/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5770 - auc: 0.7466 - val_loss: 0.5778 - val_auc: 0.7450 - lr: 0.0010\n",
            "Epoch 74/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5769 - auc: 0.7467 - val_loss: 0.5773 - val_auc: 0.7452 - lr: 0.0010\n",
            "Epoch 75/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5768 - auc: 0.7470 - val_loss: 0.5771 - val_auc: 0.7453 - lr: 0.0010\n",
            "Epoch 76/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5767 - auc: 0.7467 - val_loss: 0.5773 - val_auc: 0.7451 - lr: 0.0010\n",
            "Epoch 77/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5766 - auc: 0.7468 - val_loss: 0.5768 - val_auc: 0.7453 - lr: 0.0010\n",
            "Epoch 78/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5763 - auc: 0.7470 - val_loss: 0.5773 - val_auc: 0.7453 - lr: 0.0010\n",
            "Epoch 79/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5761 - auc: 0.7469 - val_loss: 0.5766 - val_auc: 0.7453 - lr: 0.0010\n",
            "Epoch 80/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5762 - auc: 0.7471 - val_loss: 0.5764 - val_auc: 0.7456 - lr: 0.0010\n",
            "Epoch 81/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5759 - auc: 0.7470 - val_loss: 0.5763 - val_auc: 0.7454 - lr: 0.0010\n",
            "Epoch 82/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5758 - auc: 0.7471 - val_loss: 0.5762 - val_auc: 0.7454 - lr: 0.0010\n",
            "Epoch 83/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5756 - auc: 0.7471 - val_loss: 0.5760 - val_auc: 0.7454 - lr: 0.0010\n",
            "Epoch 84/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5755 - auc: 0.7472 - val_loss: 0.5760 - val_auc: 0.7456 - lr: 0.0010\n",
            "Epoch 85/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5757 - auc: 0.7471 - val_loss: 0.5758 - val_auc: 0.7454 - lr: 0.0010\n",
            "Epoch 86/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5752 - auc: 0.7473 - val_loss: 0.5775 - val_auc: 0.7457 - lr: 0.0010\n",
            "Epoch 87/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5752 - auc: 0.7473 - val_loss: 0.5755 - val_auc: 0.7456 - lr: 0.0010\n",
            "Epoch 88/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5752 - auc: 0.7472 - val_loss: 0.5754 - val_auc: 0.7454 - lr: 0.0010\n",
            "Epoch 89/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5749 - auc: 0.7472 - val_loss: 0.5754 - val_auc: 0.7455 - lr: 0.0010\n",
            "Epoch 90/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5747 - auc: 0.7473 - val_loss: 0.5755 - val_auc: 0.7458 - lr: 0.0010\n",
            "Epoch 91/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5747 - auc: 0.7474 - val_loss: 0.5761 - val_auc: 0.7456 - lr: 0.0010\n",
            "Epoch 92/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5745 - auc: 0.7474 - val_loss: 0.5749 - val_auc: 0.7455 - lr: 0.0010\n",
            "Epoch 93/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5746 - auc: 0.7473 - val_loss: 0.5748 - val_auc: 0.7456 - lr: 0.0010\n",
            "Epoch 94/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5744 - auc: 0.7472 - val_loss: 0.5747 - val_auc: 0.7457 - lr: 0.0010\n",
            "Epoch 95/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5743 - auc: 0.7473 - val_loss: 0.5752 - val_auc: 0.7458 - lr: 0.0010\n",
            "Epoch 96/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5741 - auc: 0.7473 - val_loss: 0.5746 - val_auc: 0.7456 - lr: 0.0010\n",
            "Epoch 97/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5741 - auc: 0.7472 - val_loss: 0.5745 - val_auc: 0.7459 - lr: 0.0010\n",
            "Epoch 98/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5741 - auc: 0.7471 - val_loss: 0.5743 - val_auc: 0.7457 - lr: 0.0010\n",
            "Epoch 99/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5739 - auc: 0.7473 - val_loss: 0.5749 - val_auc: 0.7459 - lr: 0.0010\n",
            "Epoch 100/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5739 - auc: 0.7473 - val_loss: 0.5746 - val_auc: 0.7457 - lr: 0.0010\n",
            "Epoch 101/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5737 - auc: 0.7474 - val_loss: 0.5746 - val_auc: 0.7457 - lr: 0.0010\n",
            "Epoch 102/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5737 - auc: 0.7475 - val_loss: 0.5739 - val_auc: 0.7459 - lr: 0.0010\n",
            "Epoch 103/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5736 - auc: 0.7474 - val_loss: 0.5739 - val_auc: 0.7458 - lr: 0.0010\n",
            "Epoch 104/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5735 - auc: 0.7474 - val_loss: 0.5739 - val_auc: 0.7457 - lr: 0.0010\n",
            "Epoch 105/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5735 - auc: 0.7475 - val_loss: 0.5739 - val_auc: 0.7458 - lr: 0.0010\n",
            "Epoch 106/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5733 - auc: 0.7474 - val_loss: 0.5736 - val_auc: 0.7457 - lr: 0.0010\n",
            "Epoch 107/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5734 - auc: 0.7474 - val_loss: 0.5735 - val_auc: 0.7457 - lr: 0.0010\n",
            "Epoch 108/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5733 - auc: 0.7474 - val_loss: 0.5734 - val_auc: 0.7457 - lr: 0.0010\n",
            "Epoch 109/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5730 - auc: 0.7475 - val_loss: 0.5734 - val_auc: 0.7459 - lr: 0.0010\n",
            "Epoch 110/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5732 - auc: 0.7475 - val_loss: 0.5734 - val_auc: 0.7458 - lr: 0.0010\n",
            "Epoch 111/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5729 - auc: 0.7475 - val_loss: 0.5733 - val_auc: 0.7459 - lr: 0.0010\n",
            "Epoch 112/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5728 - auc: 0.7476 - val_loss: 0.5734 - val_auc: 0.7458 - lr: 0.0010\n",
            "Epoch 113/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5728 - auc: 0.7475 - val_loss: 0.5731 - val_auc: 0.7459 - lr: 0.0010\n",
            "Epoch 114/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5727 - auc: 0.7474 - val_loss: 0.5732 - val_auc: 0.7458 - lr: 0.0010\n",
            "Epoch 115/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5727 - auc: 0.7474 - val_loss: 0.5728 - val_auc: 0.7459 - lr: 0.0010\n",
            "Epoch 116/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5725 - auc: 0.7475 - val_loss: 0.5739 - val_auc: 0.7459 - lr: 0.0010\n",
            "Epoch 117/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5725 - auc: 0.7476 - val_loss: 0.5727 - val_auc: 0.7459 - lr: 0.0010\n",
            "Epoch 118/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5726 - auc: 0.7473 - val_loss: 0.5726 - val_auc: 0.7460 - lr: 0.0010\n",
            "Epoch 119/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5725 - auc: 0.7476 - val_loss: 0.5738 - val_auc: 0.7462 - lr: 0.0010\n",
            "Epoch 120/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5723 - auc: 0.7475 - val_loss: 0.5726 - val_auc: 0.7461 - lr: 0.0010\n",
            "Epoch 121/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5724 - auc: 0.7475 - val_loss: 0.5726 - val_auc: 0.7460 - lr: 0.0010\n",
            "Epoch 122/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5722 - auc: 0.7477 - val_loss: 0.5732 - val_auc: 0.7460 - lr: 0.0010\n",
            "Epoch 123/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5718 - auc: 0.7476 - val_loss: 0.5725 - val_auc: 0.7458 - lr: 2.0000e-04\n",
            "Epoch 124/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5718 - auc: 0.7477 - val_loss: 0.5723 - val_auc: 0.7460 - lr: 2.0000e-04\n",
            "Epoch 125/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5718 - auc: 0.7476 - val_loss: 0.5726 - val_auc: 0.7460 - lr: 2.0000e-04\n",
            "Epoch 126/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5718 - auc: 0.7477 - val_loss: 0.5723 - val_auc: 0.7460 - lr: 2.0000e-04\n",
            "Epoch 127/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5718 - auc: 0.7476 - val_loss: 0.5724 - val_auc: 0.7461 - lr: 2.0000e-04\n",
            "Epoch 128/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5718 - auc: 0.7477 - val_loss: 0.5723 - val_auc: 0.7459 - lr: 2.0000e-04\n",
            "Epoch 129/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5717 - auc: 0.7478 - val_loss: 0.5723 - val_auc: 0.7462 - lr: 2.0000e-04\n",
            "Epoch 130/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5717 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7459 - lr: 4.0000e-05\n",
            "Epoch 131/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7459 - lr: 4.0000e-05\n",
            "Epoch 132/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5723 - val_auc: 0.7461 - lr: 4.0000e-05\n",
            "Epoch 133/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 4.0000e-05\n",
            "Epoch 134/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7460 - lr: 4.0000e-05\n",
            "Epoch 135/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7460 - lr: 4.0000e-05\n",
            "Epoch 136/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 8.0000e-06\n",
            "Epoch 137/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7478 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 8.0000e-06\n",
            "Epoch 138/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 8.0000e-06\n",
            "Epoch 139/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 8.0000e-06\n",
            "Epoch 140/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 8.0000e-06\n",
            "Epoch 141/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 1.6000e-06\n",
            "Epoch 142/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 1.6000e-06\n",
            "Epoch 143/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 1.6000e-06\n",
            "Epoch 144/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7460 - lr: 1.6000e-06\n",
            "Epoch 145/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 1.6000e-06\n",
            "Epoch 146/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 3.2000e-07\n",
            "Epoch 147/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 3.2000e-07\n",
            "Epoch 148/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 3.2000e-07\n",
            "Epoch 149/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 3.2000e-07\n",
            "Epoch 150/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 3.2000e-07\n",
            "Epoch 151/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 6.4000e-08\n",
            "Epoch 152/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 6.4000e-08\n",
            "Epoch 153/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 6.4000e-08\n",
            "Epoch 154/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 6.4000e-08\n",
            "Epoch 155/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 6.4000e-08\n",
            "Epoch 156/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 1.2800e-08\n",
            "Epoch 157/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 1.2800e-08\n",
            "Epoch 158/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 1.2800e-08\n",
            "Epoch 159/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 1.2800e-08\n",
            "Epoch 160/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 1.2800e-08\n",
            "Epoch 161/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 2.5600e-09\n",
            "Epoch 162/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 2.5600e-09\n",
            "Epoch 163/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 2.5600e-09\n",
            "Epoch 164/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7477 - val_loss: 0.5722 - val_auc: 0.7461 - lr: 2.5600e-09\n",
            "Fold 4 NN: 0.74596\n",
            "Training fold 5\n",
            "CV 5/5\n",
            "(480000, 100) (480000,)\n",
            "(120000, 100) (120000,)\n",
            "Epoch 1/1000\n",
            "235/235 [==============================] - 4s 13ms/step - loss: 0.6931 - auc: 0.4998 - val_loss: 0.6931 - val_auc: 0.5000 - lr: 0.0010\n",
            "Epoch 2/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.6931 - auc: 0.5000 - val_loss: 0.6931 - val_auc: 0.5000 - lr: 0.0010\n",
            "Epoch 3/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6929 - auc: 0.5069 - val_loss: 0.6920 - val_auc: 0.5411 - lr: 0.0010\n",
            "Epoch 4/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.6733 - auc: 0.7049 - val_loss: 0.6409 - val_auc: 0.7263 - lr: 0.0010\n",
            "Epoch 5/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.6184 - auc: 0.7263 - val_loss: 0.6074 - val_auc: 0.7290 - lr: 0.0010\n",
            "Epoch 6/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.6036 - auc: 0.7313 - val_loss: 0.6032 - val_auc: 0.7306 - lr: 0.0010\n",
            "Epoch 7/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.6012 - auc: 0.7331 - val_loss: 0.6018 - val_auc: 0.7318 - lr: 0.0010\n",
            "Epoch 8/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.6001 - auc: 0.7340 - val_loss: 0.6004 - val_auc: 0.7327 - lr: 0.0010\n",
            "Epoch 9/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5991 - auc: 0.7348 - val_loss: 0.5994 - val_auc: 0.7335 - lr: 0.0010\n",
            "Epoch 10/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5983 - auc: 0.7355 - val_loss: 0.5986 - val_auc: 0.7342 - lr: 0.0010\n",
            "Epoch 11/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5975 - auc: 0.7360 - val_loss: 0.5978 - val_auc: 0.7348 - lr: 0.0010\n",
            "Epoch 12/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5968 - auc: 0.7366 - val_loss: 0.5971 - val_auc: 0.7354 - lr: 0.0010\n",
            "Epoch 13/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5961 - auc: 0.7372 - val_loss: 0.5964 - val_auc: 0.7359 - lr: 0.0010\n",
            "Epoch 14/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5955 - auc: 0.7376 - val_loss: 0.5958 - val_auc: 0.7365 - lr: 0.0010\n",
            "Epoch 15/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5949 - auc: 0.7380 - val_loss: 0.5953 - val_auc: 0.7369 - lr: 0.0010\n",
            "Epoch 16/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5944 - auc: 0.7384 - val_loss: 0.5954 - val_auc: 0.7373 - lr: 0.0010\n",
            "Epoch 17/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5938 - auc: 0.7388 - val_loss: 0.5939 - val_auc: 0.7377 - lr: 0.0010\n",
            "Epoch 18/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5932 - auc: 0.7392 - val_loss: 0.5934 - val_auc: 0.7382 - lr: 0.0010\n",
            "Epoch 19/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5927 - auc: 0.7396 - val_loss: 0.5927 - val_auc: 0.7384 - lr: 0.0010\n",
            "Epoch 20/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5922 - auc: 0.7399 - val_loss: 0.5922 - val_auc: 0.7388 - lr: 0.0010\n",
            "Epoch 21/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5918 - auc: 0.7402 - val_loss: 0.5921 - val_auc: 0.7394 - lr: 0.0010\n",
            "Epoch 22/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5912 - auc: 0.7406 - val_loss: 0.5913 - val_auc: 0.7396 - lr: 0.0010\n",
            "Epoch 23/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5907 - auc: 0.7410 - val_loss: 0.5914 - val_auc: 0.7398 - lr: 0.0010\n",
            "Epoch 24/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5904 - auc: 0.7411 - val_loss: 0.5905 - val_auc: 0.7403 - lr: 0.0010\n",
            "Epoch 25/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5899 - auc: 0.7414 - val_loss: 0.5899 - val_auc: 0.7405 - lr: 0.0010\n",
            "Epoch 26/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5894 - auc: 0.7417 - val_loss: 0.5891 - val_auc: 0.7409 - lr: 0.0010\n",
            "Epoch 27/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5889 - auc: 0.7420 - val_loss: 0.5887 - val_auc: 0.7411 - lr: 0.0010\n",
            "Epoch 28/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5885 - auc: 0.7422 - val_loss: 0.5884 - val_auc: 0.7413 - lr: 0.0010\n",
            "Epoch 29/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5880 - auc: 0.7425 - val_loss: 0.5881 - val_auc: 0.7415 - lr: 0.0010\n",
            "Epoch 30/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5877 - auc: 0.7427 - val_loss: 0.5876 - val_auc: 0.7420 - lr: 0.0010\n",
            "Epoch 31/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5872 - auc: 0.7429 - val_loss: 0.5875 - val_auc: 0.7421 - lr: 0.0010\n",
            "Epoch 32/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5869 - auc: 0.7431 - val_loss: 0.5865 - val_auc: 0.7422 - lr: 0.0010\n",
            "Epoch 33/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5864 - auc: 0.7434 - val_loss: 0.5861 - val_auc: 0.7425 - lr: 0.0010\n",
            "Epoch 34/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5861 - auc: 0.7434 - val_loss: 0.5857 - val_auc: 0.7426 - lr: 0.0010\n",
            "Epoch 35/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5857 - auc: 0.7438 - val_loss: 0.5853 - val_auc: 0.7429 - lr: 0.0010\n",
            "Epoch 36/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5855 - auc: 0.7437 - val_loss: 0.5850 - val_auc: 0.7433 - lr: 0.0010\n",
            "Epoch 37/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5849 - auc: 0.7440 - val_loss: 0.5846 - val_auc: 0.7433 - lr: 0.0010\n",
            "Epoch 38/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5846 - auc: 0.7441 - val_loss: 0.5842 - val_auc: 0.7436 - lr: 0.0010\n",
            "Epoch 39/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5843 - auc: 0.7442 - val_loss: 0.5840 - val_auc: 0.7437 - lr: 0.0010\n",
            "Epoch 40/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5839 - auc: 0.7445 - val_loss: 0.5843 - val_auc: 0.7438 - lr: 0.0010\n",
            "Epoch 41/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5837 - auc: 0.7446 - val_loss: 0.5831 - val_auc: 0.7439 - lr: 0.0010\n",
            "Epoch 42/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5833 - auc: 0.7448 - val_loss: 0.5829 - val_auc: 0.7441 - lr: 0.0010\n",
            "Epoch 43/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5830 - auc: 0.7449 - val_loss: 0.5825 - val_auc: 0.7443 - lr: 0.0010\n",
            "Epoch 44/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5827 - auc: 0.7451 - val_loss: 0.5828 - val_auc: 0.7444 - lr: 0.0010\n",
            "Epoch 45/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5824 - auc: 0.7451 - val_loss: 0.5818 - val_auc: 0.7445 - lr: 0.0010\n",
            "Epoch 46/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5821 - auc: 0.7452 - val_loss: 0.5815 - val_auc: 0.7447 - lr: 0.0010\n",
            "Epoch 47/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5818 - auc: 0.7453 - val_loss: 0.5812 - val_auc: 0.7448 - lr: 0.0010\n",
            "Epoch 48/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5814 - auc: 0.7453 - val_loss: 0.5809 - val_auc: 0.7448 - lr: 0.0010\n",
            "Epoch 49/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5811 - auc: 0.7455 - val_loss: 0.5806 - val_auc: 0.7451 - lr: 0.0010\n",
            "Epoch 50/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5808 - auc: 0.7457 - val_loss: 0.5804 - val_auc: 0.7453 - lr: 0.0010\n",
            "Epoch 51/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5806 - auc: 0.7457 - val_loss: 0.5803 - val_auc: 0.7451 - lr: 0.0010\n",
            "Epoch 52/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5803 - auc: 0.7459 - val_loss: 0.5798 - val_auc: 0.7456 - lr: 0.0010\n",
            "Epoch 53/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5800 - auc: 0.7461 - val_loss: 0.5798 - val_auc: 0.7457 - lr: 0.0010\n",
            "Epoch 54/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5798 - auc: 0.7460 - val_loss: 0.5792 - val_auc: 0.7458 - lr: 0.0010\n",
            "Epoch 55/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5795 - auc: 0.7460 - val_loss: 0.5789 - val_auc: 0.7461 - lr: 0.0010\n",
            "Epoch 56/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5792 - auc: 0.7461 - val_loss: 0.5787 - val_auc: 0.7462 - lr: 0.0010\n",
            "Epoch 57/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5790 - auc: 0.7461 - val_loss: 0.5787 - val_auc: 0.7464 - lr: 0.0010\n",
            "Epoch 58/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5788 - auc: 0.7460 - val_loss: 0.5784 - val_auc: 0.7468 - lr: 0.0010\n",
            "Epoch 59/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5787 - auc: 0.7461 - val_loss: 0.5780 - val_auc: 0.7470 - lr: 0.0010\n",
            "Epoch 60/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5783 - auc: 0.7462 - val_loss: 0.5779 - val_auc: 0.7469 - lr: 0.0010\n",
            "Epoch 61/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5780 - auc: 0.7464 - val_loss: 0.5776 - val_auc: 0.7471 - lr: 0.0010\n",
            "Epoch 62/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5779 - auc: 0.7464 - val_loss: 0.5773 - val_auc: 0.7474 - lr: 0.0010\n",
            "Epoch 63/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5777 - auc: 0.7463 - val_loss: 0.5771 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 64/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5775 - auc: 0.7465 - val_loss: 0.5772 - val_auc: 0.7477 - lr: 0.0010\n",
            "Epoch 65/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5772 - auc: 0.7463 - val_loss: 0.5769 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 66/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5771 - auc: 0.7464 - val_loss: 0.5768 - val_auc: 0.7476 - lr: 0.0010\n",
            "Epoch 67/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5768 - auc: 0.7465 - val_loss: 0.5765 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 68/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5767 - auc: 0.7465 - val_loss: 0.5765 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 69/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5765 - auc: 0.7466 - val_loss: 0.5760 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 70/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5763 - auc: 0.7466 - val_loss: 0.5761 - val_auc: 0.7477 - lr: 0.0010\n",
            "Epoch 71/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5764 - auc: 0.7464 - val_loss: 0.5758 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 72/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5760 - auc: 0.7467 - val_loss: 0.5758 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 73/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5758 - auc: 0.7466 - val_loss: 0.5754 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 74/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5759 - auc: 0.7468 - val_loss: 0.5752 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 75/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5754 - auc: 0.7468 - val_loss: 0.5752 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 76/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5755 - auc: 0.7468 - val_loss: 0.5758 - val_auc: 0.7477 - lr: 0.0010\n",
            "Epoch 77/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5752 - auc: 0.7467 - val_loss: 0.5748 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 78/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5752 - auc: 0.7466 - val_loss: 0.5747 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 79/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5751 - auc: 0.7466 - val_loss: 0.5751 - val_auc: 0.7482 - lr: 0.0010\n",
            "Epoch 80/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5748 - auc: 0.7468 - val_loss: 0.5749 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 81/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5748 - auc: 0.7468 - val_loss: 0.5745 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 82/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5745 - auc: 0.7468 - val_loss: 0.5742 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 83/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5745 - auc: 0.7467 - val_loss: 0.5750 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 84/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5743 - auc: 0.7469 - val_loss: 0.5741 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 85/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5743 - auc: 0.7469 - val_loss: 0.5745 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 86/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5742 - auc: 0.7468 - val_loss: 0.5753 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 87/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5740 - auc: 0.7469 - val_loss: 0.5740 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 88/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5739 - auc: 0.7468 - val_loss: 0.5734 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 89/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5737 - auc: 0.7468 - val_loss: 0.5735 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 90/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5737 - auc: 0.7471 - val_loss: 0.5736 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 91/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5737 - auc: 0.7470 - val_loss: 0.5737 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 92/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5734 - auc: 0.7469 - val_loss: 0.5753 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 93/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5734 - auc: 0.7470 - val_loss: 0.5733 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 94/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5733 - auc: 0.7470 - val_loss: 0.5729 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 95/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5730 - auc: 0.7471 - val_loss: 0.5728 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 96/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5730 - auc: 0.7472 - val_loss: 0.5726 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 97/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5729 - auc: 0.7471 - val_loss: 0.5727 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 98/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5729 - auc: 0.7470 - val_loss: 0.5728 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 99/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5728 - auc: 0.7472 - val_loss: 0.5728 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 100/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5727 - auc: 0.7472 - val_loss: 0.5724 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 101/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5728 - auc: 0.7473 - val_loss: 0.5722 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 102/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5724 - auc: 0.7472 - val_loss: 0.5728 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 103/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5724 - auc: 0.7471 - val_loss: 0.5729 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 104/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5724 - auc: 0.7471 - val_loss: 0.5725 - val_auc: 0.7482 - lr: 0.0010\n",
            "Epoch 105/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5722 - auc: 0.7472 - val_loss: 0.5720 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 106/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5722 - auc: 0.7473 - val_loss: 0.5718 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 107/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5720 - auc: 0.7473 - val_loss: 0.5718 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 108/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5720 - auc: 0.7472 - val_loss: 0.5720 - val_auc: 0.7482 - lr: 0.0010\n",
            "Epoch 109/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5719 - auc: 0.7473 - val_loss: 0.5718 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 110/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5717 - auc: 0.7471 - val_loss: 0.5716 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 111/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5719 - auc: 0.7472 - val_loss: 0.5716 - val_auc: 0.7481 - lr: 0.0010\n",
            "Epoch 112/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7475 - val_loss: 0.5715 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 113/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5715 - auc: 0.7473 - val_loss: 0.5714 - val_auc: 0.7478 - lr: 0.0010\n",
            "Epoch 114/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7474 - val_loss: 0.5714 - val_auc: 0.7482 - lr: 0.0010\n",
            "Epoch 115/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5716 - auc: 0.7473 - val_loss: 0.5712 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 116/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5713 - auc: 0.7472 - val_loss: 0.5713 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 117/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5713 - auc: 0.7474 - val_loss: 0.5717 - val_auc: 0.7482 - lr: 0.0010\n",
            "Epoch 118/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5713 - auc: 0.7474 - val_loss: 0.5710 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 119/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5712 - auc: 0.7474 - val_loss: 0.5735 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 120/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5712 - auc: 0.7474 - val_loss: 0.5710 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 121/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5710 - auc: 0.7472 - val_loss: 0.5709 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 122/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5712 - auc: 0.7474 - val_loss: 0.5713 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 123/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5711 - auc: 0.7473 - val_loss: 0.5708 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 124/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5711 - auc: 0.7472 - val_loss: 0.5709 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 125/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5709 - auc: 0.7472 - val_loss: 0.5708 - val_auc: 0.7480 - lr: 0.0010\n",
            "Epoch 126/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5709 - auc: 0.7474 - val_loss: 0.5709 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 127/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5707 - auc: 0.7474 - val_loss: 0.5708 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 128/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5708 - auc: 0.7475 - val_loss: 0.5726 - val_auc: 0.7479 - lr: 0.0010\n",
            "Epoch 129/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5704 - auc: 0.7477 - val_loss: 0.5705 - val_auc: 0.7479 - lr: 2.0000e-04\n",
            "Epoch 130/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5704 - auc: 0.7476 - val_loss: 0.5705 - val_auc: 0.7479 - lr: 2.0000e-04\n",
            "Epoch 131/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5704 - auc: 0.7475 - val_loss: 0.5705 - val_auc: 0.7477 - lr: 2.0000e-04\n",
            "Epoch 132/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5704 - auc: 0.7477 - val_loss: 0.5705 - val_auc: 0.7478 - lr: 2.0000e-04\n",
            "Epoch 133/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5703 - auc: 0.7475 - val_loss: 0.5707 - val_auc: 0.7479 - lr: 2.0000e-04\n",
            "Epoch 134/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5703 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7479 - lr: 2.0000e-04\n",
            "Epoch 135/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5703 - auc: 0.7476 - val_loss: 0.5705 - val_auc: 0.7477 - lr: 4.0000e-05\n",
            "Epoch 136/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5703 - auc: 0.7477 - val_loss: 0.5705 - val_auc: 0.7480 - lr: 4.0000e-05\n",
            "Epoch 137/1000\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.5703 - auc: 0.7477 - val_loss: 0.5705 - val_auc: 0.7480 - lr: 4.0000e-05\n",
            "Epoch 138/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5703 - auc: 0.7476 - val_loss: 0.5705 - val_auc: 0.7480 - lr: 4.0000e-05\n",
            "Epoch 139/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 4.0000e-05\n",
            "Epoch 140/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 8.0000e-06\n",
            "Epoch 141/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 8.0000e-06\n",
            "Epoch 142/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 8.0000e-06\n",
            "Epoch 143/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7477 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 8.0000e-06\n",
            "Epoch 144/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7477 - lr: 8.0000e-06\n",
            "Epoch 145/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7477 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 1.6000e-06\n",
            "Epoch 146/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 1.6000e-06\n",
            "Epoch 147/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 1.6000e-06\n",
            "Epoch 148/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 1.6000e-06\n",
            "Epoch 149/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 1.6000e-06\n",
            "Epoch 150/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7477 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 3.2000e-07\n",
            "Epoch 151/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7477 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 3.2000e-07\n",
            "Epoch 152/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 3.2000e-07\n",
            "Epoch 153/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 3.2000e-07\n",
            "Epoch 154/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 3.2000e-07\n",
            "Epoch 155/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 6.4000e-08\n",
            "Epoch 156/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 6.4000e-08\n",
            "Epoch 157/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 6.4000e-08\n",
            "Epoch 158/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 6.4000e-08\n",
            "Epoch 159/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 6.4000e-08\n",
            "Epoch 160/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 1.2800e-08\n",
            "Epoch 161/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 1.2800e-08\n",
            "Epoch 162/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 1.2800e-08\n",
            "Epoch 163/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 1.2800e-08\n",
            "Epoch 164/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 1.2800e-08\n",
            "Epoch 165/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 2.5600e-09\n",
            "Epoch 166/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 2.5600e-09\n",
            "Epoch 167/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 2.5600e-09\n",
            "Epoch 168/1000\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.5702 - auc: 0.7476 - val_loss: 0.5704 - val_auc: 0.7478 - lr: 2.5600e-09\n",
            "Fold 5 NN: 0.74785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ipcn7cnBHjrN",
        "outputId": "d6d18c6d-9d46-411c-cedb-96f3674fb44f"
      },
      "source": [
        "test_predictions_nn.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(540000,)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwIX6P3MbQgj"
      },
      "source": [
        "sub=pd.read_csv(\"/content/drive/MyDrive/빅데이터 분석 실습/[실전 과제] Kaggle playground/sample_submission.csv\")\n",
        "sub['target']=test_predictions_nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuUfoTYGbvtC"
      },
      "source": [
        "sub.to_csv(\"submission_2.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpxPxUu9byUK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
